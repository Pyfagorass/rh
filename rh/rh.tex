\documentclass[openany]{book}
%\documentclass[11pt,draft]{article}   % uncomment this and comment out the above line for *fast* typesetting (no images)


\usepackage{endnotes}
\usepackage{float}

\usepackage{wrapfig}


\usepackage{fancybox}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}

\usepackage{url}

\usepackage{makeidx}\makeindex

\DeclareMathOperator{\Gap}{Gap}
\DeclareMathOperator{\Li}{Li}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

\newcommand{\mycaption}[1]{\begin{quote}{\bf Figure: } \large #1\end{quote}}

\newcommand{\ill}[3]{%
   \begin{figure}[H]%
   \vspace{-2ex}
   \centering%
   \includegraphics[width=#2\textwidth]{illustrations/#1}%
   \caption{#3}%
   \vspace{-2ex}
    \end{figure}}

\newcommand{\illtwo}[4]{%
   \begin{figure}[H]\centering%
   \includegraphics[width=#3\textwidth]{illustrations/#1}$\qquad$\includegraphics[width=#3\textwidth]{illustrations/#2}%
   \caption{#4}%
    \end{figure}}

\newcommand{\illthree}[5]{%
   \begin{figure}[H]%
\centering%
   \includegraphics[width=#4\textwidth]{illustrations/#1}$\qquad$\includegraphics[width=#4\textwidth]{illustrations/#2}$\qquad$\includegraphics[width=#4\textwidth]{illustrations/#3}%
   \caption{#5}%
    \end{figure}}

%%%% Theoremstyles
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{hypothesis}[theorem]{Hypothesis}
\newtheorem{conjecture}[theorem]{Conjecture}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{question}[theorem]{Question}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{alg}[theorem]{Algorithm}
\newtheorem{openproblem}[theorem]{Open Problem}

%\theoremstyle{remark}
\newtheorem{goal}[theorem]{Goal}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{remarks}[theorem]{Remarks}
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}



%\hoffset=-0.05\textwidth
%\textwidth = 1.1\textwidth
%\voffset=-0.05\textheight
%\textheight = 1.1\textheight
%
% Set equal margins on book style
\setlength{\oddsidemargin}{53pt}
\setlength{\evensidemargin}{53pt}
\setlength{\marginparwidth}{57pt}
\setlength{\footskip}{30pt}

% Dutch style of paragraph formatting, i.e. no indents. 
\setlength{\parskip}{1.3ex plus 0.2ex minus 0.2ex}
\setlength{\parindent}{0pt}

\setcounter{tocdepth}{0}

%\textheight = 9 in
%\oddsidemargin = 0.0 in
%\evensidemargin = 0.0 in
%\topmargin = 0.0 in
%\headheight = 0.0 in
%\headsep = 0.0 in
%\parskip = 0.2in
%\parindent = 0.0in


\def\GL{\mathrm{GL}}
\def\PGL{\mathrm{PGL}}
\def\PSL{\mathrm{PSL}}
\def\GSP{\mathrm{GSP}}
\def\Z{\mathrm{Z}}
\def\Q{\mathrm{Q}}
\def\Gal{\mathrm{Gal}}
\def\Hom{\mathrm{Hom}}
\def\Ind{\mathrm{Ind}}
\def\End{\mathrm{End}}
\def\Aut{\mathrm{Aut}}
\def\loc{\mathrm{loc}}
\def\glob{\mathrm{glob}}
\def\Kbar{{\bar K}}
\def\D{{\mathcal D}}
\def\L{{\mathcal L}}
\def\R{{\mathcal R}}
\def\G{{\mathcal G}}
\def\W{{\mathcal W}}
\def\H{{\mathcal H}}
\def\OH{{\mathcal OH}}



\newcommand{\RH}{Riemann Hypothesis\index{Riemann Hypothesis}}


\title{\Huge What is Riemann's Hypothesis?\\
July 2010 Draft\\
{\Large This is still VERY VERY Rough and Incomplete!}}
\date{}
\author{\Large Barry Mazur \and \Large William Stein \and \vspace{20ex}\\
%\begin{center}
%A formula that counts the prime numbers ...
%\includegraphics[width=.65\textwidth]{illustrations/Rk_50_2_100}
%... built out of their spectrum:
% cover = use fractal trace with params 0.3 -0.1 0 0 
\includegraphics[width=\textwidth]{illustrations/cover}
%$$
%f(t) = -\sum_{\text{prime powers }p^n}{\frac{\log(p)}{p^{n/2}}}\cos(t\log(p^n))
%$$ 
%\end{center}
}


\usepackage[hypertex]{hyperref}

\begin{document}

\maketitle



% Remove parskip for toc
\setlength{\parskip}{0ex plus 0.5ex minus 0.2ex}

\tableofcontents


% Dutch style of paragraph formatting, i.e. no indents.
\setlength{\parskip}{1.3ex plus 0.2ex minus 0.2ex}

\chapter*{\label{foreword}Foreword}

The \RH{} is one of the great unsolved problems of
mathematics and the reward of \$1,000,000 of {\em Clay Mathematics
  Institute} prize money awaits the person who solves it. But---with
or without money---its resolution is crucial for our understanding of
the nature of numbers.

There are at least four full-length books recently published, written
for a general audience, that have the \RH{} as their main
topic.  A reader of these books will get a fairly rich picture of the
personalities engaged in the pursuit, and of related mathematical and
historical issues.\endnote{See, e.g., {\em The Music of the Primes} by Marcus du Sautoy.}
     
This is {\em not} the mission of the book that you now hold in your
hands. We aim---instead---to explain, in as direct a manner as
possible and with the least mathematical background required, what
this problem is all about and why it is so important. For even before
anyone proves this {\em hypothesis} to be true (or false!), just
getting familiar with it and with some of the the ideas behind it, is
exciting.  Moreover, this hypothesis is of crucial importance in a
wide range of mathematical fields; for example it is a
confidence-booster for computational mathematics: even if the Riemann
Hypothesis is never proved, its truth gives us an excellent sense of
how long certain computer programs will take to run, which, in some
cases, gives us the assurance we need to initiate a computation that
might take weeks or even months to complete.

Here is how the
Princeton mathematician Peter Sarnak described the broad impact the
Riemann Hypothesis has had: 
``The Riemann hypothesis is the central problem and it implies many,
many things. One thing that makes it rather unusual in mathematics
today is that there must be over five hundred papers---somebody should
go and count---which start `Assume the Riemann hypothesis,' and
the conclusion is fantastic. And those [conclusions] would then become
theorems ... With this one solution you would have proven five hundred
theorems or more at once.'' % This is from page 106 of the Borwein book.


Our book is profusely illustrated, containing over 125 figures, diagrams,
and pictures that accompany the text.  There are comparatively fewer
mathematical equations in Part~\ref{part1}.   For the more adventurous readers, we have
also created at \url{http://wstein.org/rh} something we call {\em worksheets}
that enable the reader to use the free Sage mathematical
software to experiment with the parameters for
the ranges of data illustrated, so as to get an even more
vivid sense of how the numbers ``behave.''  Also Sage code is available
at the above webpage that can be used to recreate every diagram
in this book. 
 
So, what {\it is} the Riemann Hypothesis?  Below is a {\it first
  description} of what it is about. The task of our book is to develop
the following boxed paragraph into a fuller explanation and to
convince you of the importance and beauty of the mathematics it
represents.  We will be offering, throughout our book, a number of
different---but equivalent---ways of precisely formulating this
hypothesis (we display these in boxes).  When we say that two
mathematical statements are ``equivalent" we mean that, given the
present state of mathematical knowledge, we can prove that if either
one of those statements is true, then the other is true. The endnotes
will guide the reader to the mathematical literature that establishes
these equivalences.
      
      \begin{center}
       \shadowbox{ \begin{minipage}{0.9\textwidth}
\mbox{}       \vspace{0.2ex}
       \begin{center}{\bf\large What {\em sort} of Hypothesis is the \RH{}?}\end{center}
       \medskip

Consider the seemingly innocuous series of questions:

\begin{quote}
How many prime numbers  (2, 3, 5, 7, 11, 13, 17, â€¦ ) are there less than 100?\\
How many less than 10,000?\\
How many less than 1,000,000?\vspace{-1ex}\\

More generally, how many primes are there less than any given number $X$?
\end{quote}

Riemann proposed, a century and half ago, a strikingly
simple-to-describe ``very good approximation'' to the number of
primes less than a given number $X$. We now
see that if we could prove this {\em Hypothesis of Riemann} we would have
the key to a wealth of powerful mathematics. Mathematicians are eager
to find that key.


\vspace{1ex}
\end{minipage}}
      \end{center}
      
      % I got this from http://www.math.harvard.edu/history/bott/
% Barry says there  is a good picture in the book
% "the Index Theorem" that Yau published.
\ill{raoulbott}{0.25}{Raoul Bott (1923--2005)\label{fig:bott}}


A famous mathematician, Raoul Bott, once
said---giving advice to some young mathematicians---that whenever one
reads a mathematics book or article, or goes to a math lecture, one
should aim to come home with something very specific (it can be small,
but should be {\em specific}) that has application to a wider class of
mathematical problem than was the focus of the text or lecture.  If we
were to suggest some possible {\em specific} items to come home with,
after read our book, three key phrases -- {\bf prime numbers}, {\bf
  square-root accurate}, and {\bf distribution} -- would head the
list. As for words of encouragement to think hard about the first of
these, i.e., prime numbers, we can do no better than to quote a
paragraph of Don Zagier's classic 12-page exposition, {\em The First
  50 Million Prime Numbers}:
    
% I took this myself when we were hiking in Oberwolfach once...
\ill{zagier}{.25}{Don Zagier}
                   
                   
\begin{quote}                      
  ``There are two facts about the distribution of prime numbers of
  which I hope to convince you so overwhelmingly that they will be
  permanently engraved in your hearts. The first is that, [they are]
  the most arbitrary and ornery objects studied by mathematicians:
  they grow like weeds among the natural numbers, seeming to obey no
  other law than that of chance, and nobody can predict where the next
  one will sprout. The second fact is even more astonishing, for it
  states just the opposite: that the prime numbers exhibit stunning
  regularity, that there are laws governing their behavior, and that
  they obey these laws with almost military precision.''
\end{quote}


\part{The \RH{}\label{part1}}

\numberwithin{equation}{chapter}
\numberwithin{figure}{chapter}
\numberwithin{table}{chapter}
 
\chapter[Thoughts about numbers]{Thoughts about numbers: ancient, medieval, and modern}

If we are to believe the ancient Greek philosopher Aristotle the early
Pythagoreans thought that the principles governing Number are ``the
principles of all things,'' the concept of Number being more basic than
{\em earth, air, fire, or water}, which were according to ancient tradition
the four building blocks of matter. To think about number is to get
close to the architecture of ``what is.''

So, how far along are we in our thoughts about numbers?


\ill{descartes}{.25}{Ren\'e Descartes}

The French philosopher and mathematician Ren\'e Descartes, almost four
centuries ago, expressed the hope that there soon would be ``almost
nothing more to discover in geometry.'' Contemporary physicists dream
of a final theory\endnote{Link to Weinberg's book {\em Dreams of a
    Final Theory: The Search for the Fundamental Laws of Nature}, by
  Steven Weinberg (New York: Pantheon Books, 1992)}.  But despite its
venerability and its great power and beauty, the pure mathematics of
numbers may still be in the infancy of its development, with depths to
be explored as endless as the human soul, and never a final theory.


% This is from http://en.wikipedia.org/wiki/File:Frans_Hals_-_Portret_van_Ren%C3%A9_Descartes.jpg
% There is a higher resolution scan available there



\ill{dulcinea1}{.2}{Don Quixote and ``his'' Dulcinea del Toboso}

Numbers are obstreperous things. Don Quixote encountered this when he
requested that the ``bachelor'' compose a poem to his lady Dulcinea del
Toboso, the first letters of each line spelling out her name. The
``bachelor'' found




\begin{quote}
  ``a great difficulty in their composition because the number of
  letters in her name was $17$, and if he made four Castilian stanzas
  of four octosyllabic lines each, there would be one letter too many,
  and if he made the stanzas of five octosyllabic lines each, the ones
  called {\em d{\'e}cimas} or {\em redondillas,} there would be three
  letters too few...''
\end{quote}
  
``It must fit in, however, you do it,'' pleaded Quixote, not willing to
grant the imperviousness of the number $17$ to division.

% http://www.jus.uio.no/sisu/don_quixote.miguel_de_cervantes/60.html#2068




{\em Seventeen} is indeed a prime number: there is no way of factoring
it as the product of smaller numbers, and this accounts---people tell
us---for its occurrence in some phenomena of nature, as when
the $17$-year cicadas all emerged to celebrate a ``reunion'' of some
sort in our fields and valleys.

\ill{cicada}{.3}{Cicadas emerge every 17 years}
% from http://biology.clc.uc.edu/steincarter/cicadas.htm




Prime numbers, despite their {\em primary} position in our modern
understanding of number, were not specifically doted over in the
ancient literature before Euclid, at least not in the literature that
has been preserved. Primes are mentioned as a class of numbers in the
writings of Philolaus (a predecessor of Plato); they are not mentioned
specifically in the Platonic dialogues, which is surprising 
given the intense interest Plato had in mathematical developments; and
they make an occasional appearance in the writings of Aristotle, which
is not surprising, given Aristotle's emphasis on the distinction
between the {\em composite} and the {\em incomposite}. ``The
incomposite is prior to the composite,'' writes Aristotle in Book 13 of
the Metaphysics.

% from http://en.wikipedia.org/wiki/File:Euklid-von-Alexandria_1.jpg           
\ill{euclid}{.3}{Euclid}           

But, until Euclid, prime numbers seem not to have been singled out as
{\em the} extraordinary mathematical concept, central to any deep
understanding of numerical phenomena, that they are now understood to
be.
       



There is an extraordinary wealth of established truths about numbers;
these truths provoke sheer awe for the beautiful complexity of prime
numbers. But each of the important new discoveries we make give rise
to a further richness of questions, educated guesses, heuristics,
expectations, and unsolved problems.
            
            
            
\chapter{What are prime numbers?}\label{ch:what_are_primes}

\noindent {\em Primes as atoms. } To begin from the beginning, think
of the operation of multiplication as a bond that ties numbers
together: the equation $2\times 3= 6$ invites us to imagine the number
$6$ as (a molecule, if you wish) built out of its smaller constituents
$2$ and $3$.  Reversing the procedure, if we start with a whole
number, say $6$ again, we may try to factor it (that is, express it as
a product of smaller whole numbers) and, of course, we would
eventually, if not immediately, come up with $6 = 2\times 3$ and
discover that $2$ and $3$ factor no further; the numbers $2$ and $3$,
then, are the indecomposable entities (atoms, if you wish) that
comprise our number.  

\ill{factor_tree_6}{.3}{The number $6 = 2\times 3$}
            

By definition, a {\bf prime number}
(colloquially, {\em a prime}) is a whole number, bigger than $1$, that
cannot be factored into a product of two smaller whole numbers. So,
$2$ and $3$ are the first two prime numbers. The next number along the
line, $4$, is not prime, for $4= 2\times 2$; the number after that,
$5$, is. Primes are, multiplicatively speaking, the building blocks
from which all numbers can be made. A fundamental theorem of
arithmetic tells us that any number (bigger than $1$) can be factored
as a product of primes, and the factorization is {\em unique} except
for rearranging the order of the primes. 

For example, if you try to factor the number $12$ as a product of
smaller numbers---ignoring the order of the factors---there are two
ways to begin to do this:
$$
  12 = 2 \times 6 \qquad\text{ and }\qquad   12 = 3 \times 4
$$
But neither of these ways is a full factorization of $12$, for both
$6$ and $4$ are not prime, so can be themselves factored, and in each
case after changing the ordering of the factors we arrive at:
$$
   12= 2 \times 2 \times 3.
$$
\illtwo{factor_tree_12}{factor_tree_12b}{0.3}{Factorizations of $12$}

If you try to factor the number $300$, there are many
ways to begin:
$$
  300= 30\times 10\qquad\text{or}\qquad 300 = 6 \times 50
$$
and there are various other starting possibilities. But if you
continue the factorization (``climbing down'' any one of the possible
``factoring trees'') to the bottom, where every factor is a prime
number as in Figure~\ref{fig:factor300}, you always end up with the
same collection of prime numbers:
                 $$300 = 2^2\times 3\times 5^2.$$   

\illtwo{factor_tree_300_a}{factor_tree_300_b}{.47}
{Factor trees that illustrates the factorization of 300 as a product of primes.\label{fig:factor300}}

\ill{factor_tree_big}{1}{Factorization tree for the product of the primes up to $29$.\label{factor.tree.big}}
 
                        
The \RH{} probes the question: how intimately can we know
prime numbers, those {\em atoms} of multiplication?  Prime numbers are
an important part of our daily lives.  For example, anytime we visit a
website and purchase something online, prime numbers having hundreds of
decimal digits are used to keep our bank transactions private.  This
ubiquitous use to which these giant primes are put depends upon a very
simple principle: it is much easier to multiply numbers together than
to factor them. If you had to factor, say, the number $143$ you might
scratch your head for a few minutes before discovering that $143$ is
$11\times 13$. But if you had to multiply $11$ by $13$ you would do it
straightaway.  Offer two primes, say, $P$ and $Q$ each with a few hundred 
digits, to your computing machine and ask it to multiply them
together: you will get their product $N = P\times Q$ with its hundreds of digits
in less than a microsecond. But present that number $N$ to any
current desktop computer, and ask it to factor $N$, and the computer
will (almost certainly) fail to do the task. The safety of much
encryption depends upon this guaranteed\endnote{Nobody has ever
  published a {\em proof} that there is no fast way to factor
  integers.  This is an article of ``faith'' among some
  cryptographers.} failure!

If we were latter-day number-phenomenologists we might revel in the
discovery and proof that
$$
  p=2^{43,112,609}-1 = 3164702693\ldots\ldots\ldots6697152511
$$ 
is a prime number, this number having $12,\!978,\!189$ digits!  This
prime, which was discovered on August 23, 2008 by the GIMPS project
(\url{http://www.mersenne.org/}), was the first prime ever found with
more than ten million digits.

Now $2^{43,112,609}-1$ is quite a hefty number! Suppose someone came
up to you saying ``surely $p = 2^{43,112,609}-1$ is the largest prime
number!'' (which it is not) how might you convince that person that
he or she is wrong?

Here is a neat---and, we hope, convincing---strategy to show there are
prime numbers even larger than $p = 2^{43,112,609} - 1$. Imagine
forming the following humungous number: let $M$ be the product of all
prime numbers up to and including $p = 2^{43,11,2609} - 1$.  Now go
one further than $M$ by taking the next number $N=M+1$.
 

OK, even though this number $N$ is wildly large, it is either a prime
number itself---which would mean that there would indeed be a prime
number larger than $p=2^{43,112,609} - 1$, namely $N$; or in any event it is
surely divisible by some prime number, call it $P$.

Here, now, is a way of seeing that this $P$ is bigger than $p$: Since
every prime number smaller than or equal to $p$ divides $M$, these
prime numbers cannot divide $N= M+1$ (since they divide $M$ evenly, if
you tried to divide $N=M+1$ by any of them you would get a remainder
of $1$).  So, since $P$ does divide $N$ it must not be any of the
smaller prime numbers: $P$ is therefore a prime number bigger than $p=
2^{43,112,609}-1$.

This strategy, by the way, is not very new: it is, in fact, well over
two thousand years old, since it already occurred in Euclid's {\em
  Elements}. The Greeks did know that there are infinitely many prime
numbers and they showed it via the same method as we showed that our
$p = 2^{43,112,609} - 1$ is not the largest prime number.

Here is the argument again, given very succinctly: Suppose there are
only finitely many primes $p_1, \ldots, p_n$.  Let $n=p_1 p_2 \cdots
p_n + 1$.  Then $n$ is divisible by some prime, but no $p_i$ divides
$n$, which is contrary to our assumption that $p_1, \ldots, p_n$ is
the complete list of primes.


You can think of this strategy as a simple game that you can
play. Start with any finite bag of prime numbers (say the bag that
only contains one prime, the prime $2$). Now each ``move'' of the game
consists of multiplying together all the primes you have in your bag
to get a number $M$, then adding $1$ to $M$ to get the even larger
number $N=M+1$, then factoring $N$ into prime number factors, and then
including all those new prime numbers in your bag. Euclid's proof
gives us that we will---with each move of this game---be finding more
prime numbers: the bag will increase. After, say, a million moves our
bag will be guaranteed to contain more than a million prime numbers.

For example, starting the game with your bag containing
only one prime number $2$, here is how your bag grows with after
successive moves of the game:

$\mbox{}\qquad\{2\}$
\newline
$\mbox{}\qquad\{2,3\}$
\newline
$\mbox{}\qquad\{2,3, 7\}$
\newline
$\mbox{}\qquad\{2,3, 7, 43\}$
\newline
$\mbox{}\qquad\{2,3, 7, 43, 13, 139\}$
\newline
$\mbox{}\qquad\{2,3, 7, 43, 13, 139, 3263443\}$
\newline
$\mbox{}\qquad\{2,3, 7, 43, 13, 139, 3263443,  547, 607, 1033, 31051\}$
\newline
$\mbox{}\qquad\{2,3, 7, 43, 13, 139, 3263443,  547, 607, 1033, 31051, 29881, 67003,\\
\mbox{}\qquad\qquad\qquad 9119521, 6212157481\}$
\newline
\mbox{}\qquad{}etc.\endnote{The sequence of prime numbers we find by this
procedure is discussed in more detail with references at
Sloane's table of integer sequences:
\url{http://www.research.att.com/~njas/sequences/A126263}.}


Though there are infinitely many primes, actually finding them is a
major challenge.  In the 1990s, the Electronic Frontier Foundation
\url{http://www.eff.org/awards/coop} offered a \$100,000 cash reward
to the first group to find a prime with at least 10,000,000 decimal
digits (the record prime $p$ above won this prize\endnote{%
See \url{http://www.eff.org/press/archives/2009/10/14-0}.
Also the 46th Mersenne prime was declared by Time Magazine to be
one of the top 50 best ``inventions'' of 2008: \url{http://www.time.com/time/specials/packages/article/0,28804,1852747_1854195_1854157,00.html}.}), and offers
another \$150,000 cash prize to the first group to find a prime with
at least 100,000,000 decimal digits.


The number $p= 2^{43,112,609} - 1$  is the largest prime we know, where by
``know'' we mean that we know it so explicitly that we can {\em
  compute} things about it.  For example, the last two digits of $p$
are both 1 and the sum of the digits of $p$ is 58,416,637.  Of course $p$ is not
the largest prime number since there are infinitely many primes, e.g.,
the next prime $q$ after $p$ is a prime.  But there is no known way
to efficiently compute anything interesting about $q$.  For example,
what is the last digit of $q$ in its decimal expansion?



%\ill{sieve_boxes_100}{0.7}{Sieve of Eratosthenes\label{fig:erat}}

\chapter{Sieves}\label{ch:sieves}

Eratosthenes, the mathematician from Cyrene (and later, librarian at
Alexandria) explained how to {\em sift} the prime numbers from the
series of all numbers: in the sequence of numbers,
$$2\ \ 3\ \ 4 \ \ 5\ \ 6\ \ 7\ \ 8\ \ 9\ \ 10\ \ 11
\ \ 12\ \ 13\ \ 14\ \ 15\ \ 16\ \ 17\ \ 18\ \ 19\ \ 20\ \ 21\ \ 22\ \ 23\ \ 24\ \ 25\ \ 26,$$
for example, start by circling the $2$ and crossing out all the other
multiples of $2$.  Next, go back to the beginning of our sequence of
numbers and circle the first number that is neither circled nor
crossed out (that would be, of course, the $3$), then cross out all
the other multiples of $3$.  This gives the pattern: go back again to
the beginning of our sequence of numbers and circle the first number
that is neither circled nor crossed out; then cross out all of its
other multiples.  Repeat this pattern until all the numbers in our
sequence are either circled, or crossed out, the circled ones being
the primes.



In Figures~\ref{fig:erat2}--\ref{fig:erat2357} we use the primes $2$,
$3$, $5$, and finally $7$ to sieve out the primes up to 100, where instead of
crossing out multiples we grey them out, and instead of circling
primes we color their box red.

\ill{sieve100-2}{.8}{Using the prime 2 to sieve for primes up to 100\label{fig:erat2}}
Since all the even numbers greater than two are
eliminated as being composite numbers and not primes they appear as
gray in Figure~\ref{fig:erat2}, but none of the odd numbers are eliminated so they
still appear in white boxes.

\ill{sieve100-3}{.8}{Using the primes 2 and 3 to sieve for primes up to 100\label{fig:erat23}}
\ill{sieve100-5}{.8}{Using the primes 2,3,5 to sieve for primes up to 100\label{fig:erat235}}
Looking at Figure~\ref{fig:erat235}, we see that for all but
six numbers up to 100 we have  (after sieving by 2,3, and 5) determined
which are primes and which composite.

\ill{sieve100-7}{.8}{Using the primes 2,3,5,7 to sieve for primes up to 100\label{fig:erat2357}}

Finally, as in Figure~\ref{fig:erat2357}, 
sieving by 2,3,5, and 7 we have determined which are primes for all but two numbers.

% GIVEN WHAT IS ABOVE, THIS IS SILLY.
%Especially if you have had little experience with math, may I suggest
%that you actually follow Eratosthenes' lead, and perform the repeated
%circling and crossing-out to watch the primes up to 100 emerge, intriguingly
%staggered through our sequence of numbers,
%$$2\ \ 3\ \ \bullet \ \ 5\ \ \bullet\ \ 7\ \ \bullet\ \ \bullet\ \ \bullet\ \ 11
%\ \ \bullet\ \ 13\ \ \bullet\ \ \bullet\ \ \bullet\ \ 17\ \ \bullet\ \ 19\ \ \bullet\ \ \bullet\ \
%\bullet\
%\ 23\ \ \bullet\ \ \bullet\ \ \bullet\ \  \bullet\ \ \bullet\ 29,\dots $$ 
%

%


\chapter[Questions about primes]{Questions about primes that any person might ask}

We become quickly stymied when we ask quite elementary questions about
the spacing of the infinite series of prime numbers.

For example, {\em are there infinitely many pairs of primes whose
  difference is $2$?}  The sequence of primes seems to be rich in such
pairs $$5-3 =2,\ \ \ 7-5=2,\ \ \ 13-11=2,\ \ \ 19-17 =2,$$ and we know
that there are loads more such pairs\endnote{For example, according to
\url{http://www.research.att.com/~njas/sequences/A007508} there are
$10,\!304,\!185,\!697,\!298$ such pairs less than
$10,\!000,\!000,\!000,\!000,\!000$.} but the answer to our question,
{\em are there infinitely many?}, is not known. {\em Are there
  infinitely many pairs of primes whose difference is $4$?}  Answer:
equally unknown. {\em Is every even number greater than $2$ a sum of
  two primes?}  Answer: unknown. {\em Are there infinitely many primes
  which are $1$ more than a perfect square?}  Answer: unknown.




Remember the Mersenne prime $p= 2^{43,112,609}-1$? and how we showed
that there is a prime $P$ larger than that? Suppose, though, someone
asked us whether there was a {\it Mersenne Prime} larger than this
$p$: that is, {\em is there a prime number of the form $$2^{\rm some\
  prime\ number}-1$$ bigger than $p= 2^{43,112,609}-1$?} Answer:  We don't
know. It is possible that there are infinitely many Mersenne primes
but we're far from being able to answer such questions.

% from http://www.york.ac.uk/depts/maths/histstat/people/mersenne.gif
\ill{mersenne}{.3}{Mersenne}



{\em Is there some neat formula giving the next prime?} More
specifically, {\em If I give you a number $N$, say $N=$ one million,
  and ask you for the first number after $N$ that is prime, is there a
  method that answers that question without, in some form or other,
  running through each of the successive odd numbers after $N$ rejecting
  the nonprimes until the first prime is encountered?}  Answer:
unknown.


%%%%%%%%%%%%%%%

One can think of many ways of ``getting at'' some understanding of the
placement of prime numbers among all number.  Up to this point we have
been mainly just counting them, trying to answer the question ``how
many primes are there up to $X$?''  and we have begun to get some feel
for the numbers behind this question, and especially for the current
``best guesses'' about estimates.
   

What is wonderful about this subject is that people attracted to it
cannot resist asking questions that lead to interesting, and sometimes
surprising numerical experiments. Moreover, given our current state of
knowledge, many of the questions that come to mind are still
unapproachable: we don't yet know enough about numbers to answer them.
But {\it asking interesting questions} about the mathematics that you
are studying is a high art, and is probably a necessary skill to
acquire, in order to get the most enjoyment---and understanding---from
mathematics.  So, we offer this challenge to you:

Come up with with your own question about primes that
 \begin{itemize}
 \item     is interesting to you,
  \item    is not a question whose answer is known to you,
 \item     is not a question that you've seen before; or at least not exactly,
  \item    is a question about which you can begin to make numerical investigations. 
 \end{itemize}
If you are having trouble coming up with a question, read on for more
examples that  provide further motivation.
 
\chapter{Further questions about primes} 

Let us, for variety, dice the question differently by concentrating on
the {\em gaps} between one prime and the next, rather than the tally
of all primes. Of course, it is no fun at all to try to guess how many
pairs of primes $p, q$ there are with gap $q-p$ equal to to a fixed
odd number.  The fun, though, begins in earnest if you ask for pairs
of primes with difference equal to $2$ (these being called {\em twin
  primes}) for it has long been guessed that there are infinitely many
such pairs of primes, but no one has been able to prove this yet.

\begin{quote} The largest known twin primes are 
$$65516468355\cdot 2^{333333} \pm 1$$
They have $100355$ digits and were found by Kaiser and Klahnin.\endnote{See \url{http://primes.utm.edu/largest.html\#twin} for the top ten
largest known twin primes.}
\end{quote}


Similarly, it interesting to consider primes $p$ and $q$
with difference $4$, or $8$, or---in fact---any even number
$2k$. That is, people have guessed that there are infinitely many
pairs of primes with difference $4$, with difference $6$, etc. but
none of these guesses have yet been proved. 




   
So, define $$\Gap_{k}(X)$$ to be the number of pairs of primes of
successive primes $(p,q)$ with $q<X$ that have ``gap $k$'' (i.e., such
that their difference $q-p$ is $k$).  Here $p$ is a prime, $q>p$ is a
prime, and there are no primes between $p$ and $q$.  For example,
$\Gap_2(10) = 2$, since the pairs $(3,5)$ and $(5,7)$ are the
pairs less than $10$ with gap $2$.  See Table~\ref{tab:gap} for
various values of $\Gap_{k}(X)$ and Figure~\ref{fig:primegapdist}
for the distribution of prime gaps for $X=10^7$.
   
   
\begin{table}[H]\centering
\caption{Values of $\Gap_{k}(X)$ \label{tab:gap}}
\vspace{1em}

\begin{tabular}{|l|c|c|c|c|}\hline
$X$ & $\Gap_{2}(X)$ & $\Gap_{4}(X)$& $\Gap_{6}(X)$ & $\Gap_{8}(X)$\\\hline
10 & 2 & 1 & 0 & 0\\\hline
$10^2$ & 8 & 8 & 7 & 1 \\\hline
$10^3$ & 35 & 40 & 44 & 15 \\\hline
$10^4$ & 205 & 202 & 299 & 101 \\\hline
$10^5$ & 1224 & 1215 & 1940 & 773 \\\hline
$10^6$ & 8169 & 8143 & 13549 & 5569 \\\hline
$10^7$ & 58980 & 58621 & 99987 & 42352 \\\hline
\end{tabular}
\end{table}  



\ill{primegapdist}{1}{Frequency histogram showing the distribution of prime gaps of size $\leq 50$ for all primes up to $10^7$.  Six is the most popular gap in this data.\label{fig:primegapdist}}

   
   
   Here is yet another question with deals with
the spacing of prime numbers that we can't answer.
  
{\em Racing Gap $2$,  Gap $4$, Gap $6$, and Gap $8$ against each other:} 
  
\begin{quote}
  Challenge: As $X$ tends to infinity which of $\Gap_2(X),
  \Gap_4(X), \Gap_6(X),$ or $\Gap_8(X)$ do you think that will grow
  faster? How much would you bet on the truth of your guess?\endnote{%
  Hardy and Littlewood give a nice conjectural answer to such questions about
  gaps between primes.  See [reference Guy's book].}
\end{quote}
  



Here is a curious question that you can easily begin to check out for
small numbers. We know, of course, that the {\em even} numbers and the
{\em odd} numbers are nicely and simply distributed: after every odd
number comes an even number, after every even, an odd, there is an
equal number of odd number as even numbers less than any given odd
number, and there may be nothing else of interest to say about the
matter.  Things change considerably, though, if we focus our
concentration on {\em multiplicatively even} numbers and {\em
  multiplicatively odd} numbers.
  
  
A {\bf multiplicatively even} number is one that can be expressed as a
product of {\em an even number of} primes; and a {\bf multiplicatively
  odd} number is one that can be expressed as a product of {\em an odd
  number of} primes.  So, any prime is multiplicatively odd, the
number $4 = 2\cdot 2$ is multiplicatively even, and so is $6=2\cdot
3$, $9=3\cdot 3$, and $10= 2\cdot 5$; but $12=2\cdot 2\cdot 4$ is
multiplicatively odd.  Table~\ref{tab:evenodddata} gives some data:
   
  \begin{table}[H] \caption{Count of multiplicatively even and odd positive numbers $\le N$\label{tab:evenodddata}}
\vspace{1ex}
\centering
 {\small
\begin{tabular}{|l|r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|}
\hline
N & 2 & 3 & 4 & 5  & 6 & 7 & 8 & 9 & 10 & 11 & 12 & 13 & 14 & 15 & 16  \\ \hline
Mult. even & 1 & 1 & 2 & 2  & 3 & 3 & 3 & 4 & 5 & 5 & 5 & 5 & 6 & 7 & 8 \\ \hline
Mult. odd & 1 & 2 & 2 & 3  & 3 & 4 & 5 & 5 & 5 & 6 & 7 & 8 & 8 & 8 & 8 \\ \hline
\end{tabular}}
\end{table}

  
   Now looking at this data, a natural, and simple, question to ask about the concept of multiplicative {\em oddness} and {\em evenness} is: 
   
   \noindent {\em Is there some  $N\ge 2$ for which there are more multiplicatively even numbers less than or equal to $N$ than multiplicatively odd ones?}

Each plot in Figure~\ref{fig:liouville} gives the number of
multiplicatively even numbers between $2$ and $N$ minus the number of
multiplicatively odd numbers between $2$ and $N$, for $N$ equal to 10,
100, 1000, 10000, 100000, and 1000000. The above question asks whether these graphs
would, for sufficiently large $N$, ever cross the $x$-axis.

 \begin{figure}[H]
\centering
\includegraphics[width=.4\textwidth]{illustrations/liouville-10}
\includegraphics[width=.4\textwidth]{illustrations/liouville-100}\\
\includegraphics[width=.4\textwidth]{illustrations/liouville-1000}
\includegraphics[width=.4\textwidth]{illustrations/liouville-10000}\\
\includegraphics[width=.4\textwidth]{illustrations/liouville-100000}
\includegraphics[width=.4\textwidth]{illustrations/liouville-1000000}\\
\caption{Racing Multiplicatively Even and Odd Numbers.\label{fig:liouville}}
\end{figure}
  
A {\em negative} response to this question---i.e., a proof that any plot as drawn in 
Figure~\ref{fig:liouville}
would imply the Riemann
Hypothesis!  In contrast to the list of previous questions, the answer
to this one is known\endnote{For more details, see P.~Borwein, {\em Sign changes
  in sums of the Liouville Function} and the nice short paper of
Norbert Wiener {\em Notes on Polya's and Turan's hypothesis concerning
  Liouville's factor} (page 765 of volume II of Wiener's Collected
Works); see also: G. Polya {\em Vershiedene Bemerkungen zur
  Zahlentheorie} Jahresbericht de Deutschen Mathematiker-Vereinigung,
{\bf 28} (1919) 31--40.}: alas, there is such an $N$.  In 1960, Lehman
showed that for $N=906,400,000$ there are $708$ more multiplicatively
even numbers up to $N$ than multiplicatively odd numbers (Tanaka found
in 1980 that the smallest $N$ 
such that there are more multiplicative even than odd numbers is
$N=906,150,257$).

%%%%%%%%%%%%%%%%%
%\begin{wrapfigure}{r}{0.2\textwidth}
%    \includegraphics[width=0.2\textwidth]{illustrations/questions}
%\end{wrapfigure}
These are questions that have been asked about primes (and we could
give bushels more), questions expressible in simple vocabulary, that
we can't answer today. We have been studying numbers for over two
millenia and yet we are indeed in the infancy of our understanding.


So we'll continue our discussion by returning to the simplest counting
question about prime numbers.  
                           
\chapter{How many primes are there?}

\ill{sieve200}{.8}{Sieving Primes up to 200}
                                                      
Slow as we are to understand primes, at the very least we can try to
count them. You can see that there are $10$ primes less than $30$, so
you might encapsulate this by saying that the chances that a number
less than $30$ is prime is $1$ in $3$.  This frequency does not
persist, though; here is some more data: There are $25$ primes less
than $100$ (so $1$ in $4$ numbers up to $100$ are prime), there are
$168$ primes less than a thousand (so we might say that among the
numbers less than a thousand the chances that one of them is prime is
roughly $1$ in $6$).
                                                           
\ill{proportion_primes_100}{1}{Graph of the proportion of primes up to $N$ for each integer $N\leq 100$}

There are 78,498 primes less than a million (so we might say that
the chances that a random choice among the first million numbers is
prime have dropped to roughly $1$ in $13$).

\illtwo{proportion_primes_1000}{proportion_primes_10000}{0.48}{Proportion of primes for $N$ up to $1000$ (left) and $10000$ (right)}

There are 455,052,512 primes less than ten billion; i.e.,
10,000,000,000 (so we might say that the chances are down to roughly
$1$ in $22$).

Primes, then, seem to be thinning out.  We return to the sifting process
we carried out earlier, and take a look at a few graphs, to get a sense of why
that might be so. There are a hundred numbers less than or equal to
$100$, a thousand numbers less than or equal to $1000$, etc.: the
shaded bar graph in Figure~\ref{fig:sieve_2_100} that looks like a regular staircase, each step the
same length as each riser, climbing up at, so to speak, a 45 degree
angle, counts all numbers up to and including~$N$.

Following Eratosthenes, we have sifted those numbers, to pan for
primes. Our first move was to throw out roughly half the numbers (the
even ones!) after the number $2$. The cross-hatched bar graph in this
figure which is, with one hiccup, a regular staircase climbing at a
smaller angle, each step twice the lengths of each riser, illustrates
the numbers that are left after one pass through Eratosthenes' sieve,
which includes, of course, all the primes. So, the chances that a
number bigger than $2$ is prime is {\em at most} $1$ in $2$.  Our
second move was to throw out a good bunch of numbers bigger than $3$.
So, the chances that a number bigger than $3$ is prime is going to be
even less.  And so it goes: with each move in our
sieving process we are winnowing the field more extensively, reducing
the chances that the later numbers are prime.

\ill{sieve_2_100}{.8}{Sieving by removing multiples of $2$ up to $100$\label{fig:sieve_2_100}}
\ill{sieve1000}{.8}{Sieving for primes up to $1000$}
          
The red curve in these figures actually counts the primes: it is the
beguilingly irregular {\em staircase of primes.}  Its height above any
number $X$ on the horizontal line records the number of primes less
than or equal to $X$, the accumulation of primes up to $X$.  Refer to
this number as $\pi(X)$. So $\pi(2)=1$, $\pi(3) = 2$, $\pi(30) = 10$; of
course, if you believed some of the data above you could plot a few
more values of $\pi(X)$, like $\pi(\text{ten billion}) = 455,052,512$.
                              
                                 
Let us accompany Eratosthenes for a few further steps in his sieving
process.  Figure~\ref{fig:sieve3_100} contains a graph of all whole
numbers up to 100 after we have removed the even numbers greater than
$2$, and the multiples of $3$ greater than $3$ itself.
                                 

\ill{sieves3_100}{.7}{Sieving out multiples of $2$ and $3$.\label{fig:sieve3_100}}


From this graph you can see that if you go ``out a way'' the
likelihood that a number is a prime is less that $1$ in $3
$. Figure~\ref{fig:sieve7_100} contains a graph of what Eratosthenes
sieve looks like up to 100 after sifting $2,3,5,$ and $7$.



\ill{sieves7_100}{.7}{Sieving out multiples of $2$, $3$, $5$, and $7$.\label{fig:sieve7_100}}


This data may begin to suggest to you that as you go further and
further out on the number line the percentage of prime numbers among
all whole numbers tends towards $0\%$ (it does).
  

To get a sense of how the primes accumulate, we will take a look at
the staircase of primes for $N= 25$ and $N=100$ in Figures~\ref{fig:staircase25}
and \ref{fig:staircase100a}.

\ill{prime_pi_25_aspect1}{.8}{Staircase of primes up to 25\label{fig:staircase25}}
\ill{prime_pi_100_aspect1}{.8}{Staircase of primes up to 100\label{fig:staircase100a}}

    
    
\chapter{Prime numbers viewed from a distance}
The striking thing about these figures is that as the numbers get
large enough, the jagged accumulation of primes, those
quintessentially discrete entities, becomes smoother and smoother, to
the eye. How strange and wonderful to watch, as our viewpoint zooms
out to larger ranges of numbers, the accumulation of primes taking on
such a smooth and elegant shape.

\illtwo{prime_pi_1000}{prime_pi_10000}{0.4}{Staircases of primes up to 1,000 and 10,000\label{fig:staircases2}}

\ill{prime_pi_100000}{.8}{Primes up to 100,000\label{fig:pn100000}.}


But don't be fooled by the seemingly smooth shape of the curve in the
last figure above: it is just as faithful a reproduction of the
staircase of primes as the typographer's art can render, for there are
thousands of tiny steps and risers in this curve, all hidden by the
thickness of the print of the drawn curve in the figure.  It is
already something of a miracle that we can approximately describe the
build-up of primes, somehow, using a {\em smooth curve}.  But {\em
  what} smooth curve?


That last question is {\em not} rhetorical. If I draw a curve with
chalk on the blackboard, this can signify a myriad of smooth
(mathematical) curves all encompassed within the thickness of the
chalk-line, all--if you wish--reasonable approximations of one
another. So, there are many smooth curves that fit the chalk-curve.
With this warning, but very much fortified by the data of Figure~\ref{fig:pn100000},
let us ask: {\em what is a smooth curve that is a reasonable
  approximation to the staircase of primes?}

\chapter{Pure and applied  mathematics}     

Everyone seems to agree that, loosely speaking, there are two types of
mathematics: {\em pure} and {\em applied}. Usually---when we judge
whether a piece of mathematics is pure or applied---this distinction
turns on whether or not the math has application to the ``outside
world,'' i.e., that {\em world} where bridges are built, where economic
models are fashioned, where computers churn away on the Internet (for
only then do we unabashedly call it {\em applied math}), or whether
the piece of mathematics will find an important place within the
context of mathematical theory (and then we label it {\em pure}).  Of
course, there is a great overlap (as we will see later, Fourier analysis plays 
a major role both in data compression and in pure mathematics).

 Moreover, many questions in
mathematics are ``hustlers'' in the sense that, at first view, what is
being requested is that some simple task be done (e.g., the question
raised in this book, {\em to find a smooth curve that is a reasonable
  approximation to the staircase of primes}).  And only as things
develop is it discovered that there are payoffs in many unexpected
directions, some of these payoffs being genuinely applied (i.e., to
the practical world), some of these payoffs being pure (allowing us
to strike behind the mask of the mere appearance of the mathematical
situation, and get at the hidden fundamentals that actually govern the
phenomena), and some of these payoffs defying such simple
classification, insofar as they provide powerful techniques in other
branches of mathematics.  The \RH{}---even in its current
unsolved state---has already shown itself to have all three types of
payoff.

The particular issue before us is, in our opinion, twofold, both
applied, and pure: can we curve-fit the ``staircase of primes'' by a
well approximating smooth curve given by a simple analytic formula?  
The story behind this alone is
marvelous, has a cornucopia of applications, and we will be telling it
below. But our curiosity here is driven by a question that is pure,
and less amenable to precise formulation: are there mathematical
concepts at the root of, and more basic than (and ``prior to,'' to
borrow Aristotle's use of the phrase,) {\em prime numbers}--concepts
that account for the apparent complexity of the nature of primes?
   
   
\chapter{A probabilistic ``first'' guess\label{sec:firstguess} }

\ill{gauss}{.3}{Gauss}

The search for such approximating curves began, in fact, two centuries
ago when Carl Friedrich Gauss defined a certain beautiful curve that,
experimentally, seemed to be an exceptionally good fit for the
staircase of primes. 

\ill{logXoverX}{.7}{Plot of $\pi(X)$ and $\log(X)/(X-1)$}

Let us denote Gauss's curve $G(X)$; it has an
elegant simple formula comprehensible to anyone who has had a tiny bit
of calculus.  If you make believe that the chances that a number $N$ is
a prime is inversely proportional to the number of digits of $N$ you
might well hit upon Gauss's curve.   
That is,
\vskip10pt
$$G(N)\hskip40pt  {\rm is\ roughly proportional to} \hskip40pt  {\frac{X}{{\rm the\ number\ of\ digits\ of\ } N}}.$$
\vskip10pt
But to describe Gauss's guess precisely we need to discuss the {\it natural logarithm} ``$\log(X)$" which is an elegant smooth function of real numbers $X$ that is roughly proportional to the number of digits of the whole number part of $X$.



\ill{log}{.8}{Plot of the natural logarithm $\log(X)$}


 Euler's famous constant $e=2.71828182\ldots$ which is the limit 
 of the sequence 
 $$\left(1+{\frac{1}{2}}\right)^2,  
       \left(1+{\frac{1}{3}}\right)^3, 
       \left(1+{\frac{1}{4}}\right)^4, \dots$$
is used in the definition of $\log$: 
\begin{center}
{\em $A = \log(X)$ is the number $A$ for which $e^A = X$.}
\end{center}
Before electronic calculators, logarithms were frequently used to 
speed up calculations, since logarithms translate difficult multiplication
problems into easier addition problems which can be done mechanically.
Such calculations use that the logarithm of a product is the sum of the logarithms
of the factors; that is, $\log(XY) = \log(X) + \log(Y)$. 
%FROM -- YURI TSCHINKEL's: "ABOUT THE COVER: ON THE DISTRIBUTION OF PRIMESÑGAUSSÕ TABLES "

% From: http://en.wikipedia.org/wiki/File:Slide_rule_scales_back.jpg
\ill{slide_rule}{.8}{A slide rule  computing $2X$ by using that $\log(2X)=\log(2)+\log(X)$\label{fig:slide_rule}}

In Figure~\ref{fig:slide_rule} the numbers printed (on each of the slidable pieces of the rule)
are spaced according to their logarithms, so that when one slides the
rule arranging it so that the printed  number $X$ on one piece lines up
with the printed number $1$ on the other, we get that for every number $Y$
printed on the first piece, the printed number on the other piece that
is  aligned with it is the product $XY$; in effect the ``slide'' adds 
$\log(X)$ to $\log(Y)$ giving $\log(XY)$.

% I found this here: http://kr.blog.yahoo.com/kimxx168/498
% It is referenced in http://www.ams.org/bull/2006-43-01/S0273-0979-05-01096-7/home.html
% but that link is broken.
\ill{gauss_tables_half}{.9}{Gauss's Letter (see Endnote~\ref{end:gauss_letter} for an English translation)\label{fig:gauss_letter}}

In 1791, when Gauss was 14 years old, he received a book that contained 
logarithms of numbers up to $7$ digits and a table of primes up to 10,009.
Years later, in a letter\endnote{\label{end:gauss_letter}
The following is an English translation of Gauss's letter (Figure~\ref{fig:gauss_letter}), taken from 
\url{http://www.ams.org/bull/2006-43-01/S0273-0979-05-01096-7/home.html}.

\begin{quote}
``My distinguished friend, 

Your remarks concerning the frequency of primes were of interest to me in more 
ways than one. You have reminded me of my own endeavors in this Þeld which 
began in the very distant past, in 1792 or 1793, after I had acquired the Lambert 
supplements to the logarithmic tables. Even before I had begun my more detailed 
investigations into higher arithmetic, one of my first projects was to turn my 
attention to the decreasing frequency of primes, to which end I counted the primes in 
several chiliads and recorded the results on the attached white pages. I soon recognized 
that behind all of its fluctuations, this frequency is on the average inversely 
proportional to the logarithm, so that the number of primes below a given bound $n$ 
is approximately equal to 
$$
\int \frac{dn}{\log(n)},
$$
where the logarithm is understood to be hyperbolic. Later on, when I became acquainted 
with the list in VegaÕs tables (1796) going up to 400,031, I extended my 
computation further, conÞrming that estimate. In 1811, the appearance of ChernauÕs 
cribrum gave me much pleasure and I have frequently (since I lack the patience 
for a continuous count) spent an idle quarter of an hour to count another chiliad 
here and there; although I eventually gave it up without quite getting through a 
million. Only some time later did I make use of the diligence of Goldschmidt to Þll 
some of the remaining gaps in the Þrst million and to continue the computation 
according to BurkhardtÕs tables. Thus (for many years now) the Þrst three million 
have been counted and checked against the integral. A small excerpt follows: ....''
\end{quote}}
written in 1849 (see Figure~\ref{fig:gauss_letter}), Gauss
claimed that as early as 1792 or 1793 he had already observed that the
density of prime numbers over intervals of numbers of a given rough
magnitude $X$ seemed to average $1/\log(X)$.  Very {\em very} roughly speaking, this
means that {\em the number of primes up to $X$ is approximately $X$ divided by
twice the number of digits of $X$}.  For example,
the number of primes less than $99$ should be roughly
$$
   \frac{99}{2\times 2} = 24.75 \approx  25,
$$   
which is pretty amazing, since the correct number of
primes up to $99$ is $25$.  The number of primes up to $999$ should
be roughly
$$
   \frac{999}{2\times 3} = 166.5 \approx  167,
$$   
which is again close, since there are 168 primes up to $1000$. 
The number of primes up to $999,\!999$ should be roughly
$$
   \frac{999999}{2\times 6} = 83333.25 \approx  83,\!333,
$$   
which is close to the correct count of $78,\!498$. 

Gauss guessed that the expected number of primes up to $X$ 
is approximated by the area under the
graph of $1/\log(X)$ from $2$ to $X$ (see Figure~\ref{fig:G}).
The area under $1/\log(X)$ up to $X=999,\!999$ is $78,\!626.43\ldots$, which
is remarkably close to the correct count $78,\!498$ of the primes
up to $999,\!999$.

\illthree{area_under_log_graph_30}{area_under_log_graph_100}{area_under_log_graph_1000}{.3}{The
  expected tally of the number of primes $\leq X$ is approximated by the
  area underneath the graph of $1/\log(X)$ from $1$ to $X$.\label{fig:G}}

           
Gauss was an inveterate computer: 
he wrote in his 1849 letter  that
there are $216,\!745$ prime numbers less than three million (This is
wrong: the actual number of these primes is $216,\!816$). Gauss's curve $G(X)$
predicted that there would be $216,\!970$ primes---a miss, Gauss
thought, by 
$$225 = 216,\!970 - 216,\!745$$ 
(but actually he was closer than he thought: the
prediction of the curve $G(X)$ missed by a mere $154 = 216,\!970  - 216,\!816$)
%; not as close as the 2004 US
%elections, but pretty close nevertheless).  
Gauss's computation brings up two queries: will this spectacular ``good
fit'' continue for arbitrarily large numbers? and, the (evidently
prior) question: what counts as a good fit?



\chapter{What is a ``good approximation''?}\label{sec:sqrterror}

If you are trying to estimate a number, say, around ten thousand, and
you get it right to within a hundred, let us celebrate this kind of
accuracy by saying that you have made an approximation with {\em
  square-root error} (${\sqrt{10,\!000}}=100$). Of course, we should
really use the more clumsy phrase ``an approximation with at worst
{\em square-root error.}''  Sometimes we'll simply refer to such
approximations as {\em good approximations.} If you are trying to
estimate a number in the millions, and you get it right to within a
thousand, let's agree that---again---you have made an approximation
with {\em square-root error} (${\sqrt{1,\!000,\!000}}=1,\!000$).
Again, for short, call this a {\em good approximation.} So, when Gauss
thought his curve missed by $226$ in estimating the number of primes
less than three million, it was well within the margin we have given
for a ``good approximation.''\endnote{If $f(x)$ and 
$g(x)$ are real-valued functions of a real variable $x$ such that
for any
$\epsilon >0$ both of them take their values between $ x^{1-\epsilon}$  
and $ x^{1+\epsilon}$ for 
$x$
sufficiently large, then say that $f(x)$ and $g(x)$ are {\bf good 
approximations of one another} if,
for any positive $\epsilon$ the absolute values of their difference 
is less than $ x^{{1\over
2}+\epsilon}$ for 
$x$
sufficiently large. The functions $\Li(X)$ and $R(X)$ of are good approximations of
one another.}

More generally, if you are trying to estimate a number that has $D$
digits and you get it almost right, but with an error that has no more
than, roughly, half that many digits, let us say, again, that you have
made an approximation with {\em square-root error} or synomymously, a
{\em good approximation}.


This rough account almost suffices for what we will be discussing
below, but to be more precise, the specific {\em gauge of accuracy}
that will be important to us is not for a mere {\em single} estimate
of a {\em single} error term, 
 
 \centerline{Error term\ \  =\ \    Exact Value\ \   -\ \   Our ``good  approximation''}
  
 
 
 \noindent but rather for {\em infinite sequences} of estimates of
 error terms. Generally, if you are interested in a numerical quantity
 $q(X)$ that depends on the real number parameter $X$ (e.g., $q(X)$
 could be $\pi(X)$, ``the number of primes $< X$'') and if you have an
 explicit candidate ``approximation,'' $q_{\rm approx}(X)$, to this
 quantity, let us say that $q_{\rm approx}(X)$ is {\bf essentially a
   square-root accurate approximation to $q(X)$} if for {\em any}
 given exponent greater than $0.5$ (you choose it: $0.501$, $0.5001$,
 $0.50001$, $\dots$ for example) and for large enough $X$---where the
 phrase ``large enough'' depends on your choice of exponent---the {\bf
   error term}---i.e., the difference between $q_{\rm approx}(X)$ and
 the {\em true} quantity, $q(X)$, is, in absolute value, less than
 $q_{\rm approx}(X)$ raised to that exponent (e.g. $< X^{0.501}$, $<
 X^{0.5001}$, etc.). Readers who know calculus and wish to have a
 technical formulation of this definition of {\em good approximation}
 might turn to the endnote $^2$ for a precise statement.
If you found the above confusing, don't worry: again, a 
square-root accurate approximation is one in which at least roughly
half the digits are correct.
 

\begin{remark}
  To get a feel for how basic the notion of {\em approximation to data
    being square root close to the true values of the data} is---and
  how it represents the ``gold standard'' of accuracy for
  approximations, consider this fable.


  Imagine that the devil had the idea of saddling a large committee of
  people with the task of finding values of $\pi(X)$ for various large
  numbers $X$.  This he did in the following somewhat ridiculous
  manner, having already worked out which numbers are prime and which
  composite, himself. Since the devil is, as everyone knows, {\em into
    the details,} he has made no mistakes: his work is entirely
  correct.  He gives each committee member a copy of the list of all
  whole numbers between $1$ and one of the large numbers $X$ in which
  he was interested, the devil having put check-marks by the numbers
  on that list that are prime numbers, and no mark by the composite
  numbers. Now each committee member would count the number of primes
  by considering each number, in turn, on their list to figure out
  whether or not there is a check beside it, and tally up the ones
  that have checks. But since they are human, they will indeed be
  making mistakes, say $0.001\%$ of the time.  Assume further that it
  is just as likely for them to make the mistake of thinking that a
  number {\em has} a check-mark that does not have one, as the
  contrary: that a number hasn't got a mark, when it does.  If many
  people are engaged in such pursuit, some of them might over-count
  $\pi(X)$; some of them might under-count it. The average error
  (over-counted or undercounted) would be proportional to~${\sqrt X}$.

\end{remark}

\chapter[What is Riemann's Hypothesis?] { What is Riemann's Hypothesis?  ({\em first formulation})\label{sec:rh1}}

 

Recall that a rough approximation to $\pi(X)$, the number of primes $<
X$,  is given by the function $X/\log(X)$; and  Gauss's guess for an
approximation to $\pi(X)$  was in terms of the area in the region
from $2$ to $X$ under the graph of $1/\log(X)$, a quantity sometimes referred to
as $\Li(X) = \int_2^X (dx/\log x)$.
``Li'' (pronounced L$\overline{\rm i}$) is short for {\em logarithmic integral}.
[[Introduction of the Li special function seems very abrupt to me.]]

Figure~\ref{fig:threeplots} contains a graph of the three functions
$\Li(X)$, $\pi(X)$, and $X/\log X$ for $X\leq 200$.
But data, no matter how impressive, may be deceiving. If you think
that  the three graphs  never cross for all large values of $X$,  and
that we have the simple relationship  $X/\log(X) < \pi(X) < Li(X)$ for
large $X$, turn to this endnote.\endnote{Very very  briefly tell people
about Littlewood's  Theorem, Skewes Number, etc.}

 
\ill{three_plots}{.9}{Plots of $\Li(X)$ (top), $\pi(X)$ (in the middle), and $X/\log(X)$ (bottom).\label{fig:threeplots}}
 
Let $X=10^{23}$.  Then\endnote{This computation of $\pi(10^{23})$ was done on a single
computer in 2007 by Oliveira e Silva,
and is the largest value of $\pi(X)$ ever computed.  See
\url{http://www.ieeta.pt/~tos/primes.html} for more details.} \label{pili_vals}
\begin{align*}
  \pi(X) &= 1925320391606803968923,\\
  \Li(X) &= 1925320391614054155138.780\ldots, \\
  X/(\log(X)-1)  &=  1924577459166813514799.7932241\ldots.\\
  \Li(X) - \pi(X) &= 7250186215.78002929687500\ldots, \\
  \sqrt{X}\log(X) &= 16747250820487.142114662460299 \ldots.
\end{align*}


Since $\Li(X)$ seems to start out impressively close in value to our
$\pi(X)$ (at least in this range) it is natural to hope that in full
generality it is {\it essentially a square root approximation} to
$\pi(X)$.  This gives us our first formulation of Riemann's
Hypothesis:

      \begin{center}
       \shadowbox{ \begin{minipage}{0.9\textwidth}
\mbox{}       \vspace{0.2ex}
       \begin{center}{\bf\large The {\bf \RH{}} (first formulation)}\end{center}
       \medskip

For any real number $X$ the number of prime numbers less than $X$ is
approximately $\Li(X)$ and this approximation is essentially square
root accurate.\endnote{In fact, $|\Li(X) - \pi(X)| \leq \sqrt{X}\log(X)$ for all $X\geq 2.01$.  See ...}
\label{rh:first}

\vspace{1ex}
\end{minipage}}
      \end{center}
 

 


\chapter{The Prime Number Theorem\label{sec:pnt}}
  
Take a look at Figure~\ref{fig:threeplots} again.  All three functions, $X/\log(X)$,
$\Li(X)$ and $\pi(X)$ are ``going to infinity with $X$'' (this means
that for any real number $R$, if $X$ is taken to be sufficiently
large, the values of these functions at $X$ will exceed $R$).

Are these functions ``going to infinity'' at {\it the same rate}?

To answer such a question, we have to know what we mean by {\it going
  to infinity at the same rate}. So, here's a definition. Two
functions, $A(X)$ and $B(X)$, that each go to infinity will be said to
{\bf go to infinity at the same rate} if their {\it ratio}
$$A(X)/B(X)$$ 
tends to $1$ as $X$ goes to infinity.

Here is one way of thinking about what it means for two functions, $A(X)$ and $B(X)$, to go to infinity at the same rate. It means that  for any
number you give us, say: a million (or a billion, or a trillion) we should be able to show that if $X$ is large enough, then the
``leftmost" million (or billion, or trillion) digits of $A(X)$ and $B(X)$ are the same. 


While we're defining things, let us say that two functions, $A(X)$
and $B(X)$, that each go to infinity {\bf go to infinity at
similar rates} if there are two positive constants $c$ and $C$
such that for $X$ sufficiently large the {\it ratio}
$$
      A(X)/B(X)
$$
is greater than $c$ and smaller than $C$.  

    \ill{similar_rates}{1.0}{The polynomials $A(X)=2 X^{2} + 3 X - 5$ (bottom)
and $B(X)=  3 X^{2} - 2 X + 1$ (top) go to infinity at similar rates.\label{fig:simrates}}



For example, two polynomials in $X$ with positive leading coefficient
{\it go to infinity at the same rate} if and only if they have the
same degrees and the same leading coefficient; they {\it go to
  infinity at similar rates} if they have the same degree.
See Figure~\ref{fig:simrates}. %where we may take $c=1/2$ and $C=1$.    

    


Now a theorem from elementary calculus tells us that the ratio of
$\Li(X)$ to $X/\log(X)$ tends to $1$ as $X$ gets larger and larger.
That is---using the definition we've just introduced--- $\Li(X)$ and
$X/\log(X)$ go to infinity at the same rate.\endnote{Give a proof
of this statement in an endnote.}


Recall (on page 10 above)  that
if $X = 10^{23}$, the top eleven digits of $\pi(X)$ and $\Li(X)$ are the same: $19253203916.$ Well, that's a good start. Can we guarantee that for  $X$ large enough, the ``top" million (or billion, or trillion) digits of $\pi(X)$ and $\Li(X)$ are the same? I.e., that these two functions go to infinity at the same rate?
  
The \RH{}, as we have just formulated it, would tell us
that the {\it difference} between $\Li(X)$ and $\pi(X)$ is pretty small
in comparison with the size of $X$. This information would imply (but
would be {\it much} more precise information than) the statement that
the {\it ratio} $\Li(X)/\pi(X)$ tends to $1$, i.e., that $\Li(X)$ and
$\pi(X)$ go to infinity at the same rate.

This last statement gives, of course, a far less precise
relationship between $\Li(X)$ and $\pi(X)$ than the \RH{}
(once it is proved!) would give us.  The advantage, though, of the
less precise statement is that it is known, and---in fact---has been
known for over a century. It goes under the name of
  

 \noindent {\bf The Prime Number Theorem:\ \ } $\Li(X)$ and $\pi(X)$ go to infinity at the same rate.
   

   Since $\Li(X)$ and $X/\log(X)$ go to infinity at the same rate, we could
   equally well have expressed the ``same" theorem by saying:

  
  \noindent {\bf The Prime Number Theorem:\ \ } $X/\log(X)$ and $\pi(X)$ go to infinity at the same rate.
   
    
   This fact is a very hard-won piece of mathematics!  It was proved
   in 1896 independently by Hadamard and de la Vall\'{e}e Poussin.

   {\bf William: here we can have more of a riff on PNT.}
        

A milestone in the history leading up to the proof of Prime
Number Theorem is the earlier work of Chebyshev [ref***]
showing that (to use the terminology we introduced)
$X/\log(X)$ and $\pi(X)$ go to infinity at similar rates.
        

The elusive \RH{}, however, is much deeper than the Prime
Number Theorem, and takes its origin from some awe-inspiring,
difficult to interpret, lines in Bernhard Riemann's magnificent 8-page
paper, ``On the number of primes less than a given magnitude,''
published in 1859. \endnote{See \url{http://www.maths.tcd.ie/pub/HistMath/People/Riemann/Zeta/}
for for the original German version and an English translation.}
 

\ill{riemann_zoom}{1}{From Riemann's 1859 Manuscript\label{fig:riemamn}}




Riemann's hypothesis, as it is currently interpreted, turns up as
relevant, as a key, again and again in different parts of the subject:
if you accept it as {\em hypothesis} you have an immensely powerful
tool at your disposal: a mathematical magnifying glass that sharpens
our focus on number theory. But it also has a wonderful protean
quality---there are many ways of formulating it, any of these
formulations being provably equivalent to any of the others.

\ill{riemann}{.3}{Bernhard Riemann (1826--1866)}


This \RH{} remains unproved to this day, and therefore is
``only a hypothesis,'' as Osiander said of Copernicus's theory, but one
for which we have overwhelming theoretical and numerical evidence in
its support.  It is the kind of conjecture that contemporary
Dutch mathematician Frans Oort might label
a {\em suffusing conjecture} in that it has unusually broad
implications: many many results are now known to follow, if the
conjecture, familiarly known as RH, is true.
A proof of RH would, therefore, fall into the {\em applied} category, given our discussion
above.  But however you classify RH, it is a central concern in
mathematics to find its proof (or, a counter-example!).  RH is 
one of the weightiest statements in all of mathematics. 


\chapter[The staircase of primes]{The {\em information} contained in the staircase of primes\label{sec:information}}
 


We have borrowed the phrase ``staircase of primes'' from the popular
book {\em The Music of Primes} by Marcus du Sautoi, for we feel that
it captures the sense that there is a deeply hidden architecture to
the graphs that compile the number of primes (up to $N$) and also
because---in a bit---we will be tinkering with this carpentry.  Before
we do so, though, let us review in Figure~\ref{fig:staircases}
what this staircase looks like, for
different ranges.

\begin{figure}[H]
\centering
\includegraphics[width=.4\textwidth]{illustrations/PN_25} 
\includegraphics[width=.4\textwidth]{illustrations/PN_100}\\ 

\includegraphics[width=.4\textwidth]{illustrations/PN_1000}
\includegraphics[width=.4\textwidth]{illustrations/PN_10000}\\ 

\includegraphics[width=.6\textwidth]{illustrations/PN_100000}


\caption{The Staircase of Primes\label{fig:staircases}}
\end{figure}

The mystery of this staircase is that the {\em information} contained
within it is---in effect---the full story of where the primes are
placed. This story seems to elude any simple description.  Can we
``tinker with'' this staircase without destroying this valuable
information?
 
 
 

\chapter[Tinkering with the staircase of primes]{Tinkering with the carpentry of the staircase of primes\label{sec:tinkering}}

 
For starters, notice that all the {\em risers} of this staircase (Figure~\ref{fig:staircases} above) have
unit length. That is, they contain no numerical information except for
their placement on the $x$-axis. So, we could distort our staircase by
changing (in any way we please) the height of each riser; and as long
as we haven't brought new risers into---or old risers out
of---existence, and have not modified their position over the
$x$-axis, we have retained all the information of our original
staircase.
   
   
A more drastic-sounding thing we could do is to judiciously add new
steps to our staircase. At present, we have a step at each prime
number $p$, and no step anywhere else. Suppose we built a staircase
with a new step not only at $x=p$ for $p$ each prime number but also at 
$x =1$ and $x=p^n$ where $p^n$ runs through all powers of prime numbers as
well. Such a staircase would have, indeed, many more steps than our
original staircase had, but, nevertheless, would retain much of the
quality of the old staircase: namely it contains within it the full
story of the placement of primes {\em and their powers}.
     
A final thing we can do is to perform a distortion of the $x$-axis
(elongating or shortening it, as we wish) in any specific way, as long
as we can perform the inverse process, and ``undistort'' it if we wish.
Clearly such an operation may have mangled the staircase, but hasn't destroyed
information irretrievably.
     
We shall perform all three of these kinds of operations eventually,
and will see some great surprises as a result.  But for now, we will
perform distortions only of the first two types.  We are about to
build a new staircase that retains the precious information we need,
but is constructed according to the following architectural plan.
 
 \begin{itemize}
 
 \item We first build a staircase that has a new step precisely at $x
   =1$, and $ x= p^n$ for every {\em prime power} $p^n$ with $n\geq
   1$. That is, there will be a new step at $x= 1,2,3,4,5,8,9,11,
   \dots$
  
 \item Our staircase starts on the ground at $x=0$ and the height of the
   riser of the step at $x=1$ will be $\log(2\pi)$. The length of the
   riser of the step at $x=p^n$ will not be $1$
   (as was the length of all risers in the old staircase of primes)
   but rather: the step at $x=p^n$ will have the height of its riser
   equal to $\log p$.  So for the first few steps listed in the
   previous item, the risers will be of length $\log(2\pi), \log
   2,\log 3,\log 2,\log 5,\log 2,\log 3,\log 11, \dots$ 
   Since $\log(p)>1$,
  these vertical dimensions lead to a steeper ascent but no great loss
  of {\em information}.
  
   Although we are not quite done with our architectural work, Figure~\ref{fig:psi} shows 
   what our staircase looks like, so far.

\illtwo{psi_9}{psi_100}{.45}{The newly constructed staircase that counts prime powers\label{fig:psi}}


  \end{itemize} 
  
  Notice that this new staircase looks, from afar, as if it were
  nicely approximated by the $45$ degree straight line, i.e., by the
  simple function $x$. In fact, we have---by this new
  architecture---a second {\em equivalent} way of formulating
  Riemann's hypothesis.   For this, let $\psi(X)$ denote the 
  function\endnote{We have $$\psi(X) =\sum_{p^n \le X}\log p$$ where the summation is over prime
powers $p^n$ that are $\le X$.}
  of $x$ whose graph is
depicted in Figure~\ref{fig:psi}.
   
          \begin{center}
       \shadowbox{ \begin{minipage}{0.9\textwidth}
\mbox{}       \vspace{0.2ex}
       \begin{center}{\bf\large The {\bf \RH{}} (second formulation)}\end{center}
       \medskip
       
       This new staircase is essentially square root close to the 45 degree
straight line; i.e., the function $\psi(X)$ is essentially square root
close to the function $f(x)=x$.
          \vspace{1ex}
   \end{minipage}}
\end{center}

\ill{psi_diag_1000}{0.5}{The newly constructed staircase is close to the 45 degree line.}

Do not worry if you do not understand why our first and second
formulations of Riemann's Hypothesis are equivalent. Our aim, in
offering the second formulation---a way of phrasing Riemann's guess
that mathematicians know to be equivalent to the first one---is to
celebrate the variety of equivalent ways we have to express Riemann's
proposed answers to the question ``How many primes are there?'' and to
point out that some formulations would reveal a startling
simplicity---not immediately apparent---to the behavior of prime
numbers, no matter how erratic primes initially appear to us to
be. After all, what could be simpler than a 45 degree straight line?
 
 


\chapter[Computer music files and prime numbers]{What do  computer music files,  
data compression, and prime numbers have to do with each other?\label{sec:fourier1}}


[[Barry: I couldn't bring myself to just delete this chapter.]]

Sounds of all sorts---and in particular the sounds of music---travel
as vibrations of air molecules at roughly 768 miles an hour. These
vibrations---fluctuations of pressure---are often represented, or
``pictured,'' by a graph where the horizontal axis corresponds to
time, and the vertical axis corresponds to pressure at that time.  The
very purest of sounds---a single sustained note---would look
something like this (called a ``sine wave'') when pictured (see
Figure~\ref{fig:sine}), so that if you fixed your position somewhere
and measured air pressure due to this sound at that position, the
peaks correspond to the times when the varying air pressure is maximal
or minimal and the zeroes to the times when it is normal pressure.

\ill{sin}{.6}{Graph of a Sine Wave\label{fig:sine}}

You'll notice that there are two features to the graph in Figure~\ref{fig:sine}.

\begin{itemize}
\item {\em The height of the peaks of this sine wave:} This height is
  referred to as the {\bf amplitude} and corresponds to the {\em
    loudness} of the sound,
\item {\em The number of peaks per second:} This number is referred to
  as the {\bf frequency} and corresponds to the {\em pitch} of the
  sound.
\end{itemize}


Of course, music is rarely---perhaps never---just given by a single
pure sustained note and nothing else. A next most simple example of a
sound would be a simple chord (say a C and an E played together on
some electronic instrument that could approximate pure notes). Its
graph would be just the {\em sum} of the graphs of each of the pure
notes (see Figures~\ref{fig:sinetwofreq} and \ref{fig:sinetwofreqsum}).
    
    
\ill{sin-twofreq}{0.6}{Graph of Two Sine Waves with Different Frequencies\label{fig:sinetwofreq}}

\ill{sin-twofreq-sum}{.6}{Graph of Sum of the Two Sine Waves with Different Frequencies\label{fig:sinetwofreqsum}}

So the picture of the changing frequencies of this chord would be
already a pretty complicated configuration.  What we have described in
these graphs are two sine waves (our C and our E) when they are played
{\em in phase} (meaning they start at the same time) but we could
also ``delay'' the onset of the E note and play them with some
different phase relationship, for example, as illustrated
in Figures~\ref{fig:sin-twofreq-phase} and \ref{fig:sum-sin-phase}.
     
\ill{sin-twofreq-phase}{.6}{\label{fig:sin-twofreq-phase}Graph of two ``sine'' waves with different phase.}

\ill{sin-twofreq-phase-sum}{.6}{Graph of the sum of the two ``sine'' waves with different frequency
and phase.\label{fig:sum-sin-phase}}

 
  So, {\em all you need} to reconstruct the chord graphed above is to
  know five numbers:
  \begin{itemize}
  \item the two frequencies---the collection of frequencies that make
    up the sound is called the {\em spectrum} of the sound,
  \item the {\em amplitudes} of each of these frequencies,
  \item the {\em phase} between them.
  
 \end{itemize}
 
 Now suppose you came across such a sound as pictured in
 Figure~\ref{fig:sum-sin-phase} and wanted to ``record it.''  Well,
 one way would be to sample the amplitude of the sound at many
 different times, as for example in Figure~\ref{fig:sum-sin-phase-sample}.
  
\ill{sin-twofreq-phase-sum-points}{.6}{Graph of sampling of a sound wave\label{fig:sum-sin-phase-sample}}

Then, fill in the rest of the points to obtain Figure~\ref{fig:sum-sin-phase-sample-fill}.

\ill{sin-twofreq-phase-sum-fill}{0.6}{Graph obtained from Figure~\ref{fig:sum-sin-phase-sample} by
filling in the rest of the points\label{fig:sum-sin-phase-sample-fill}}

But this sampling would take an enormous amount of storage space!
Current audio compact discs do their sampling 44,100 times a second to
get a reasonable quality of sound.

Another way is to simply record the {\em five} numbers: the {\em
  spectrum, amplitudes,} and {\em phase}.  Surprisingly, this seems to
be roughly the way our ear processes such a sound when we hear it.\endnote{%
We recommend to our readers that they download Dave Benson's marvelous book
{\em Music: A Mathematical Offering} from 
\url{http://www.maths.abdn.ac.uk/~bensondj/html/music.pdf}. 
This is free, and gives a beautiful account of the superb
mechanism of hearing, and of the mathematics of music.}

  Even in this simplest of examples (our pure chord: the pure note C
  played simultaneously with pure note E) the {\em efficiency of data
    compression} that is the immediate bonus of analyzing the picture
  of the chords as built {\em just} with the five numbers giving {\em
    spectrum, amplitudes,} and {\em phase} is staggering.

\ill{fourier}{0.3}{Jean Baptiste Joseph Fourier (1768--1830)}

This type of analysis, in general, is called {\em Fourier Analysis}
and is one of the glorious chapters of mathematics.  One way of
picturing {\em spectrum} and {\em amplitudes} of a sound is by a bar
graph which might be called the {\em spectral picture} of the sound,
the horizontal axis depicting frequency and the vertical one depicting
amplitude: the height of a bar at any frequency is proportional to the
amplitude of that frequency ``in'' the sound.
 
So our CE chord would have the spectral picture in
Figure~\ref{fig:ce-spectral}.
 

\illtwo{sound-ce-general_sum}{sound-ce-general_sum-blips}{.4}
       {Spectral Picture of a CE chord\label{fig:ce-spectral}}


This spectral picture ignores the phase but is nevertheless a very
good portrait of the sound.  The spectral picture of a graph gets us
to think of that graph as ``built up by the superposition of a bunch
of pure waves,'' and if the graph is complicated enough we may very well
need {\em infinitely} many pure waves to build it up!  Fourier analysis is a
mathematical theory that allows us to start with any graph---we are
thinking here of graphs that picture sounds, but any graph will 
do---and actually compute its spectral picture (and even keep track of
phases).
 
 
The operation that starts with a graph and goes to its spectral
picture that records the frequencies, amplitudes, and phases of the
pure sine waves that, together, compose the graph is called the {\em
  Fourier transform} and nowadays there are very fast procedures for
getting accurate {\em Fourier transforms} (meaning accurate spectral
pictures including information about phases) by computer.\endnote{Discuss
some good readable article on the Fast Fourier Transform algorithm; there are probably many such algorithms.}
 
 
If you pass to the worksheet at this point, you'll be able to see the
Fourier transforms of many different sounds, and experiment for
yourself.  The theory behind this operation (Fourier transform giving
us a spectral analysis of a graph) is quite beautiful, but equally
impressive is how---given the power of modern computation---you can
immediately perform this operation for yourself to get a sense of how
different wave-sounds can be constructed from the superposition of
pure tones.  

The {\em sawtooth} wave in Figure~\ref{fig:sawtooth} has a spectral picture, its Fourier transform, given in Figure~\ref{fig:sawtooth-spectrum}:
   
   \ill{sawtooth}{.7}{Graph of Sawtooth Wave\label{fig:sawtooth}}
   \ill{sawtooth-spectrum}{.6}{The Spectrum of the Sawtooth Wave Has a Spike of Height $1/k$ at 
each integer $k$\label{fig:sawtooth-spectrum}}


 
 \ill{complicated-wave}{.6}{A Complicated Sound Wave\label{fig:complicated-wave}}
 
Suppose you have a complicated sound wave, say as in
Figure~\ref{fig:complicated-wave}, and you want to record it.
Standard audio CD's record their data by intensive sampling as we
mentioned. In contrast, current mp3 audio compression technology uses
Fourier transforms plus sophisticated algorithms based on
knowledge of which frequencies the human ear can hear. 
With this, mp3 technology manages to
get a compression factor of 8--12 with little {\em perceived} loss in
quality, so that you can fit your entire music collection on your
iPod, instead of just your favorite 10 CD's.  
 
 
 \chapter{Spectra and Trigonometric Sums \label{sec:trigsums}}

As we saw in Chapter~\ref{sec:fourier1}, a pure tone can be represented by a periodic {\it sine wave}---a function of  time $f(t)$--- the equation of which might be:
$$f(t)\ \ = \ \ a\cdot \cos(b +\theta t).$$  

\vskip20pt
{\bf William:  Do you think that this deserves a picture?}
\vskip20pt
 The $\theta$ determines the {\it frequency} of the periodic wave, the larger $\theta$ is the higher the ``pitch."  The  coefficient $a$ determines the envelope of size of the periodic wave, and we  call it  the {\it amplitude} of the periodic wave.  

Sometimes we encounter functions $F(t)$ that are not pure tones, but that can be expressed as (or we might say ``decomposed into") a finite sum of pure tones, for example three  of them:

$$F(t)  = a_1\cdot \cos(b_1 +\theta_1 t) + a_2\cdot \cos(b_2 +\theta_2 t) + a_3\cdot \cos(b_3 +\theta_3 t)$$
\vskip20pt
{\bf William:  Do you think that this deserves a picture?}
\vskip20pt

   We'll refer to such functions $F(t)$ as {\it finite trigonometric sums}, because ---well---they are.
  In this example, there are three frequencies involved---i.e., $\theta_1,\theta_2,\theta_3$---and we'll say that {\it the spectrum of $F(t)$} is the set of these frequencies, i.e.,
  
  $$ {\rm The\ spectrum \ of \ } F(t) \ \ = \ \ \{\theta_1,\theta_2,\theta_3\}.$$
  
    More generally we might consider a sum of any finite number of pure cosine waves---or in a moment we'll  also see some infinite ones as well. Again, for these more general trigonometric sums, their {\it spectrum} will denote the set of frequencies that compose them.
%\end{document}
\chapter{The spectrum and the staircase of primes\label{sec:fourier_staircase}}

\ill{prime_pi_100_aspect1}{0.95}{The Staircase of Primes\label{fig:staircase100}}

 
 In view of the amazing data-compression virtues of Fourier analysis, it isn't unnatural to ask these questions:

 \begin{itemize}
  \item Is there a way of using Fourier analysis to better understand the complicated picture of the staircase
of primes?
 
  \item Does this staircase
of primes  (or, perhaps, some tinkered version of the staircase that contains the same basic information) have a {\it spectrum}?

  \item  If  such a {\it spectrum} exists, can we compute it conveniently, just as we have done for the saw-tooth wave above,
or for the major third CE chord?
 

  \item  Assuming the spectrum exists, and is computable, will our understanding of this spectrum allow us to reproduce all
the pertinent information about the placement of primes among all whole numbers, elegantly and
faithfully? 

  \item  And here is a most important question: will that spectrum show us order and organization
lurking within the staircase that we would otherwise be blind to? 
 

  \end{itemize}
   

 Strangely enough, it is towards questions like these that Riemann's Hypothesis  takes us. We began with the simple question about primes: how to count them, and are led to
ask for profound, and hidden, regularities in structure.


 In Part~\ref{part2} of this book we will---if not explain---at least hint at how the above series of questions have been answered so far, and how the \RH{} offers a surprise for the last question in this series. 
 
 But for now, let us offer a {\it sneak preview} to the ``spectrum" alluded to in these questions! We will do this by formulating two infinite trigonometric sums and exhibiting the graphs of more and more accurate finite approximations (cutoffs) of these infinite sums.
 
 \vskip10pt
 %{\bf William: We could refer our readers at this point to something on-line that allows them to make and play with the cutoffs???}
  \vskip10pt
 \begin{enumerate} \item The first infinite  trigonometric sum is a sum  of pure cosine waves with frequencies given by logarithms of powers of primes and with amplitudes that will be described below.  These graphs will have ``higher and higher peaks" concentrated more and more accurately at what we referred to as the {\it spectrum}, indicated in our pictures below by the series of vertical red lines.  
 
 \item The second infinite  trigonometric sum is a sum  of pure cosine waves with frequencies given by the spectrum of primes and with amplitudes {\it all equal to $1$}.  These graphs will have ``higher and higher peaks" concentrated more and more accurately at the {\it logarithms of powers of primes,} indicated in our pictures below by the series of vertical blue lines.
 \end{enumerate}
 
 That the series of {\it blue lines}  (i.e.,  the logarithms of powers of primes)   in our pictures below determines---via the the trigonometric sums we describe---the series of {\it red lines} (i.e., what we are calling the spectrum) and conversely is a consequence of the Riemann Hypothesis.
   \vskip10pt
\begin{enumerate} \item{\bf Towards the Spectrum, starting from (logs of) powers of the primes:}
 \vskip10pt
 To get warmed up, let's plot the positive values of the following sum of (co-)sine waves:

    \begin{align*}
   f(t) =& -{\frac{\log(2)}{2^{1/2}}}\cos(t\log(2))- {\frac{\log(3)}{3^{1/2}}}\cos(t\log(3))\\
     &\qquad -{\frac{\log(2)}{4^{1/2}}}\cos(t\log(4))-{\frac{\log(5)}{5^{1/2}}}\cos(t\log(5))
  \end{align*}



\ill{mini_phihat_even}{1}{Plot of $f(t)$}


Look at the peaks of this graph. There is nothing very impressive about them, you might think; but wait,
 for this is just a very ``early '' piece of an  expression that consists of a sum{\footnote{Here we  make use of  the Greek symbol $\sum$ as a shorthand way of expressing a sum of many terms.  We are not requesting this sum to converge.}}  of infinitely many (co-)sine waves:
$$
-\sum_{p^n}{\frac{\log(p)}{p^{n/2}}}\cos(t\log(p^n))
$$ 
 the summation being over all powers $p^n$ of all prime numbers $p$. 

Let us cut this infinite sum  off taking only finitely many terms, by choosing various 
``cut-off values'' $C$ and forming the finite sums
$$
-\sum_{p^n\leq C}{\frac{\log(p)}{p^{n/2}}}\cos(t\log(p^n)) 
$$ 
and plotting their positive values. Figures~\ref{fig:pnsum5}-\ref{fig:pnsum500} show
what we get for a few values of $C$.


 In each of the graphs, we have indicated by red vertical arrows the real numbers that give the  values of the {\it spectrum} that we will be discussing in later portions of this book. These numbers at the red vertical arrows in the graphs above, $$\theta_1, \theta_2, \theta_3, \dots$$ are the key to the staircase of primes (and if the \RH{} holds they are precisely the so-called {\it imaginary parts of the ``nontrivial" zeroes of the Riemann zeta-function}).  \vskip20pt
\begin{itemize} 
\item {\bf The sum with $p^n \le C=5$}

Here is the function $f(t)$ we displayed above; it consists in the sum of the first four terms of our infinite sum, and doesn't yet show very much ``structure":

\ill{phihat_even-5}{1}{Plot of $-\sum_{p^n\leq 5}{\frac{\log(p)}{p^{n/2}}}\cos(t\log(p^n))$ with
arrows pointing to the spectrum of the primes\label{fig:pnsum5}}\vskip20pt
\item {\bf The sum with $p^n \le C=20$} 

 Something,
 (don't you agree?)  is already beginning to happen here:
\ill{phihat_even-20}{1}{Plot of $-\sum_{p^n\leq 20}{\frac{\log(p)}{p^{n/2}}}\cos(t\log(p^n))$ with arrows pointing to the spectrum of the primes}\vskip20pt
\item {\bf The sum with $p^n \le C=50$} 

Note that the high peaks seem to be lining up more accurately with the vertical red lines. Note also that the $y$-axis has been rescaled.

\ill{phihat_even-50}{1}{Plot of $-\sum_{p^n\leq 50}{\frac{\log(p)}{p^{n/2}}}\cos(t\log(p^n))$ with arrows pointing to the spectrum of the primes}\vskip20pt
\item {\bf The sum with $p^n \le C=500$}

Here, the peaks are even sharper, and note that again they are higher; that is, we have rescaled the $y$-axis. 
\ill{phihat_even-500}{1}{Plot of $-\sum_{p^n\leq 500}{\frac{\log(p)}{p^{n/2}}}\cos(t\log(p^n))$ with arrows pointing to the spectrum of the primes\label{fig:pnsum500}}
\end{itemize}
 \vskip20pt
  We will discuss in the later parts of this book  
  \begin{itemize} \item  how this ``plays out" as we take the sums of longer  and longer  pieces of the infinite sum of cosine waves above, given by  larger and larger cutoffs $C$, \item how this spectrum  of red lines more closely matches the high peaks of the graphs of the positive values of these finite sums,\item  how these peaks are climbing higher and higher,\item   what relationship this has to the Fourier analysis of the staircase of primes,\item  and, equally importantly, what these mysterious red lines signify. 
\end{itemize}
   \vskip10pt
 \item{\bf Towards (logs of) powers of the primes, starting from the Spectrum:}
  \vskip10pt
  
  Here we will be making use of the series of numbers $$\theta_1, \theta_2, \theta_3, \dots$$ comprising what we called the {\it spectrum}. We consider the infinite trigonometric series
  $$1 + \cos(\theta_1t)+\cos(\theta_2t)+\cos(\theta_3t)+\dots$$
  
 or, using the $\sum$ notation,
 
 $$1+\sum_{\theta}\cos(\theta t)$$ where the summation is over the spectrum, $\theta=\theta_1, \theta_2, \theta_3, \dots$. Again we will consider finite cutoffs $C$ of this infinite trigonometric sum, $$1+\sum_{\theta\ < C}\cos(\theta t)$$ and plot their graphs, over various ranges.
 
    \vskip10pt
 {\bf William: Here we could simply collect some of the marvelous graphs you tabulated---e.g.,the ones currently  in chapter 24, and the new ones---without the $e^{-t/2}$ term---we really don't need it, I think; the graphs will look just as good without it. Here, of course, the vertical lines indicating the peaks at the logs of prime powers should be blue. After this sequence of graphs, we can write:}
 \end{enumerate}
  \vskip10pt
  
   This  passage---thanks to the Riemann Hypothesis--- from spectrum to prime powers and back again via consideration of the ``high peaks" in the graphs of the appropriate trigonometric sums provides a kind of visual duality emphasizing, for us, that the information inherent in the wild spacing of prime powers, is somehow ``packaged" in the series of mysterious numbers we have called the spectrum, and reciprocally, the information givden in that series of mysterious numbers is obtainable from the sequence of prime powers. 
   
  We will go into this in slightly more detail in the later chapters, but for our readers of this Part I, we hope that 
the statement of the \RH{}---admittedly as elusive as
before---has, at least, been expressed elegantly and more simply,
given our new staircase that approximates (conjecturally with {\em
  essential square root accuracy}) a 45 degree straight line.
   
We have offered two equivalent formulations of the \RH{},
both having to do with the manner in which the prime numbers are
situated among all whole numbers.

In doing this, we hope that we have convinced you that---in the words
of Don Zagier---primes seem to obey no other law than that of chance
and yet exhibit stunning regularity.  This is the end of Part~\ref{part1} of our
book, and is largely the end of our main mission, to explain---in
elementary terms---{\em what is Riemann's Hypothesis?}
    
     
For readers who have at some point studied Differential Calculus, in 
Part~\ref{part2} we shall 
go further and explain how the pursuit of
Riemann's hypothesis may provide a key to some deeper structure of the
prime numbers, and to the nature of the laws that they obey.

\chapter{To our readers of Part~\ref{part1}} 
The statement of the \RH{}---admittedly as elusive as
before---has, at least, been expressed elegantly and more simply,
given our new staircase that approximates (conjecturally with {\em
  essential square root accuracy}) a 45 degree straight line.
   
We have offered two equivalent formulations of the \RH{},
both having to do with the manner in which the prime numbers are
situated among all whole numbers.

In doing this, we hope that we have convinced you that---in the words
of Don Zagier---primes seem to obey no other law than that of chance
and yet exhibit stunning regularity.  This is the end of Part~\ref{part1} of our
book, and is largely the end of our main mission, to explain---in
elementary terms---{\em what is Riemann's Hypothesis?}
    
     
For readers who have at some point studied Differential Calculus, in 
Part~\ref{part2} we shall 
go further and explain how the pursuit of
Riemann's hypothesis may provide a key to some deeper structure of the
prime numbers, and to the nature of the laws that they obey.


\part{Distributions\label{part2}}


\chapter[Slopes of graphs that have no slopes]{How Calculus manages to find the slopes of graphs that  
have no slopes}   
   
Differential Calculus, initially the creation of Newton and/or Leibniz in the 1680s, acquaints
us with {\it slopes} of graphs of functions of a real variable.  So, to discuss this we should say a word about
 what a {\it function} is, and what its {\it graph} is.
 
 \illtwo{newton}{leibniz}{0.25}{Isaac Newton and Gottfried Leibniz created Calculus}


 A {\bf function} (let us refer to it in this discussion as $f$) is
 often described as a {\it kind of machine} that for any specific
 input numerical value $a$ will give, as output, a well-defined real
 number.
 
 This ``output number" is denoted $f(a)$ and is called {\it the
   ``value" of the function $f$ at $a$}.  For example, the {\it
   machine that adds $1$ to any number} can be thought of as the
 function $f$ whose value at any $a$ is given by the equation $f(a) =
 a+1$.  Often we choose a letter---say, $X$---to stand for a ``general
 number'' and we denote the function $f$ by the symbol $f(X)$ so that
 this symbolization allows to ``substitute for $X$ any specific number
 $a$" to get its value $f(a)$.

 The {\bf graph} of a function is a vivid visual representation of the
 function in the Euclidean plane where over every point $a$ on the
 $x$-axis you plot a point above it of ``height" equal to the value of
 the function at $a$, i.e., $f(a)$. In Cartesian coordinates, then,
 you are plotting points $(a, f(a))$ in the plane where $a$ runs
 through all real numbers.

 \ill{graph_aplusone}{0.6}{Graph of the function $f(a)=a+1$\label{fig:graph_aplusone}}

 In this book we will very often be talking about ``graphs" when we
 are also specifically interested in the functions---of which they are
 the graphs. We will use these words almost synonymously since we like
 to adopt a very visual attitude towards the behavior of the functions
 that interest us.


 \ill{graph_slope_deriv}{0.7}{A function (blue), a slope at a point
   (black straight line), and the derivative
   (red).\label{fig:graph_slope_deriv}}
         
         


Differential Calculus explains to us how to calculate slopes of graphs, and
finally, shows us the power that we then have to answer problems we
could not answer if we couldn't compute those slopes.  %See Figure~\ref{fig:graph_slope_deriv}.

Usually, in elementary Calculus classes we are called upon to compute
slopes only of smooth graphs, i.e., graphs that actually {\em have}
slopes at each of their points, such as in the illustration just
above.  What could Calculus possibly do if confronted with a graph
that has {\em jumps}, such as in Figure~\ref{fig:jump}:
$$f(x) = \begin{cases}1 & x \leq 3\\ 2 & x > 3.\end{cases}$$
(Note that for purely aesthetic reasons, we draw a vertical line at the point where the jump occurs, though technically that vertical line is not part of the graph of the function.)
 
\ill{jump}{0.5}{The graph of the function $f(x)$ above that jumps---it is $1$ up to $3$ and then $2$ after that point.\label{fig:jump}}

  
The most comfortable way to deal with the graph of such a function is
to just approximate it by a nice smooth function as in
Figure~\ref{fig:jumpsmooth}.
    
   
\ill{jump-smooth}{0.5}{A picture of a smooth graph approximating the
  graph that is $1$ up to some point $x$ and then $2$ after that
  point, the smooth graph being flat mostly.\label{fig:jumpsmooth}}


Then take the {\em derivative} of that smooth function.  Of course,
this is just an approximation, so we might try to make a better
approximation, which we do in each successive graph starting
with Figure~\ref{fig:djump1} below.


\ill{jump-smooth-deriv-7}{0.6}{A picture of the derivative of
a smooth approximation to a function that jumps.\label{fig:djump1}}

Note that---as you would expect---in the range where the initial
function is constant, its derivative is zero. In the subsequent
figures, our initial function will be {\it nonconstant} for smaller
and smaller intervals about the origin. Note also that, in our series
of pictures below, we will be successively rescaling the $y$-axis; all
our initial functions have the value $1$ for ``large" negative numbers
and the value $2$ for large positive numbers.

\ill{jump-smooth-deriv-2}{0.5}{Second picture of the derivative of
a smooth approximation to a function that jumps.\label{fig:djump2}}
\ill{jump-smooth-deriv-05}{0.5}{Third picture of the derivative of
a smooth approximation to a function that jumps.\label{fig:djump3}}
\ill{jump-smooth-deriv-01}{0.5}{Fourth picture of the derivative of
a smooth approximation to a function that jumps.\label{fig:djump4}}


Notice what is happening: as the approximation gets better and
better, the derivative will be zero mostly, with a blip at the point
of discontinuity, and the blip will get higher and higher. 
In each of these pictures,  for any interval of real numbers $[a,b]$ the total area under the red graph over that interval is equal to
\begin{center}
{\it the height of the blue graph at $x=b$}\\
minus\\
{\it  the height of the blue graph  at $x=a$}. 
\end{center}
This is a manifestation of one of the fundamental facts of life of
Calculus relating a function to its derivative:
  
\begin{quote} Given any real-valued function $F(x)$---that has a
  derivative---for any interval of real numbers $[a,b]$ the total area
  under the graph of the derivative of $F(x)$ over that interval is
  equal to $F(b)-F(a)$.
  \end{quote}  
    What happens if we take the series of figures \ref{fig:djump1}--\ref{fig:djump4}, etc. 
    {\it to the limit}?  This is quite curious: 
  
  \begin{itemize}
  \item {\bf the series of red graphs:} these are getting thinner and
    thinner and higher and higher: can we make any sense of what the
    red graph might mean in the limit (even though the only picture of
    it that we have at present makes it infinitely thin and infinitely
    high)?
  
  \item {\bf the series of blue graphs:} these are happily looking
    more and more like the tame Figure~\ref{fig:jump}.
   \end{itemize} 
  
   Each of our red graphs is the derivative of the corresponding blue
   graph. Wouldn't it be tempting to think of the limit of the red
   graphs---whatever we might construe this to be---as standing for
   the derivative of the limit of the blue graphs, i.e., of the graph
   in Figure~\ref{fig:jump}?
  
   Well, the temptation is so great that, in fact, mathematicians and
   physicists of the early twentieth century struggled to give a
   meaning to things like {\it the limit of the red graphs}---such
   things were initially called {\bf generalized functions} which
   might be considered the derivative of {\it the limit of the blue
     graphs}, i.e., of the graph of Figure~\ref{fig:jump}.
  
  
   Of course, to achieve progress in mathematics, all the concepts
   that play a role in the theory have to be unambiguously defined,
   and it took a while before {\it generalized functions} such as the
   limit of our series of red graphs had been rigorously introduced.
  
   But many of the great moments in the development of mathematics
   occur when mathematicians---requiring some concept not yet
   formalized---work with the concept tentatively, dismissing---if
   need be---mental tortures, in hopes that the experience they
   acquire by working with the concept will eventually help to put
   that concept on sure footing. For example, early mathematicians
   (Newton, Leibniz)|in replacing approximate speeds by instantaneous
   velocities by passing to limits|had to wait a while before later
   mathematicians (e.g., Weierstrass) gave a rigorous foundation for
   what they were doing.
   
   
 % From http://en.wikipedia.org/wiki/Karl_Weierstrass
 
% From http://en.wikipedia.org/wiki/Laurent_Schwartz
 \illtwo{weierstrass}{schwartz}{.3}{Karl Weierstrass (1815--1897) and Laurent Schwartz (1915--2002)} 
 
 Karl Weierstrass, who worked during the latter part of the nineteenth
 century, was known as the ``father of modern analysis.'' He oversaw
 one of the glorious moments of rigorization of concepts that were
 long in use, but never before systematically organized.  He, and
 other analysts of the time were interested in providing a rigorous
 language to talk about {\it functions} and more specifically {\it
   continuous functions} and {\it smooth} (i.e., {\it differentiable})
 functions. They wished to have a firm understanding of limits (i.e.,
 of sequences of numbers, or of functions).
 
   
 For Weierstrass and his companions, even though the functions they
 worked with needn't be smooth, or continuous, at the very least, the
 functions they studied had {\it well-defined---and usually
   finite---values}.  But our ``limit of red graphs" is not so easily
 formalized as the concepts that occupied the efforts of
 Weierstrass. Happily however, this general process of approximating
 discontinuous functions more and more exactly by smooth functions,
 and taking their derivatives to get the blip-functions as we have
 just seen in the red graphs above was eventually given a
 mathematically rigorous foundation; notably, by the French
 mathematician, Laurent Schwartz who provide a beautiful theory that
 we will not go into here, that made perfect sense of ``generalized
 functions'' such as our limit of the series of red graphs, and that
 allows mathematicians to work with these concepts with ease. These
 ``generalized functions'' are called {\it distributions} in
 Schwartz's theory.\endnote{ See
   \url{http://en.wikipedia.org/wiki/Distribution_\%28mathematics\%29}
   for more about distributions.  Also, Shwartz's explains on page 238
   of his autobiography: ``Why did we choose the name distribution?
   Because, if $\mu$ is a measure, i.e., a particular kind of
   distribution, it can be considered as a distribution of electric
   charges in the universe.  Distributions give more general types of
   electric charges, for example dipoles and magnetic distributions.
   If we consider the dipole placed at the point $a$ having magnetic
   moment $M$, we easily see that it is defined by the distribution
   $-D_M \delta_{(a)}$.  These objects occur in physics.  Deny's
   thesis, which he defended shortly after, introduced electric
   distributions of finite energy, the only ones which really occur in
   practice; these objects really are distributions, and do not
   correspond to measures.  Thus, distributions have two very
   different aspects: they are a generalization of the notion of
   function, and a generalization of the notion of distribution of
   electric charges in space. [...] Both these interpretations of
   distributions are currently used.''}




\chapter[Distributions]{Distributions: sharpening our approximating functions even if
  we have to let them shoot out to infinity\label{sec:dist}}
  


The curious {\it limit of the red graphs} of the previous section,
which you might be tempted to think of as a ``blip-function'' that
vanishes for $t$ nonzero and is somehow ``infinite'' (whatever that
means) at $0$ is an example of a {\it generalized function} (in the
sense of the earlier mathematicians) or a {\it distribution} in the
sense of Laurent Schwartz.

This particular {\it limit of the red graphs} also goes by another
name (it is officially called a Dirac $\delta$-function, the adjective
``Dirac'' being in honor of the physicist who first worked with this
concept, the ``$\delta$'' being the symbol he assigned to these
objects). The noun ``function'' should be in quotation-marks for,
properly speaking, the Dirac $\delta$-function is not---as we have
explained above---a bona fide function but rather a distribution.
      
      \ill{dirac}{0.3}{Paul Adrien Maurice Dirac (1902--1984)}

     
      Now may be a good time to summarize what the major difference is
      between {\it honest functions} and {\it generalized functions}
      or {\it distributions}.

      An honest (by which we mean {\it integrable}) function of a real
      variable $f(t)$ possesses two ``features.''


\begin{itemize}
\item {\bf It has values.}  That is, at any real number $t$, e.g., $t
  =2$ or $t =0$ or $t =\pi$ etc., our function has a definite real
  number value ($f(2)$ or $f(0)$ or $f(\pi)$ etc.) {\it and if we know
    all those values we know the function.}
     
\item {\bf It has areas under its graph.} If we are given any interval
  of real numbers, say the interval between $a$ and $b$, we can talk
  unambiguously about the area ``under'' the graph of the function
  $f(t)$ over the interval between $a$ and $b$.  That is, in the
  terminology of Integral Calculus, we can talk of {\it the integral of $f(t)$
    from $a$ to $b$}.  And in the notation of Calculus, this---thanks
  to Leibniz---is elegantly denoted
$$\int_a^bf(t)dt.$$

\end{itemize}

   \ill{oo_integral}{.8}{This figure illustrates
      $\int_{-\infty}^{\infty} f(x) dx$, which is the signed area
      between the graph of $f(x)$ and the $x$-axis, where area below
      the $x$-axis (yellow) counts negative, and area above (grey) is
      positive.}
     

In contrast, a {\it generalized function}  or  {\it distribution} 

\begin{itemize}
\item {\bf may not have ``definite values''} at all real numbers if it
  is not an honest function, nevertheless:
     
\item {\bf It has well-defined areas under portions of its ``graph."}
  If we are given any interval of real numbers, say the (open)
  interval between $a$ and $b$, we can still talk unambiguously about
  the {\it area ``under" the graph of the generalized function $D(t)$
    over the interval between $a$ and~$b$} and we will denote
  this--extending what we do in ordinary calculus---by the symbol
$$\int_a^bD(t).$$ 
\end{itemize} 

This description is important to bear in mind and it gives us a handy
way of thinking about ``generalized functions" (i.e., distributions)
as opposed to functions: when we consider an (integrable) function of
a real variable, $f(t)$, we may invoke its {\it value} at every real
number and for every interval $[a,b]$ we may consider the quantity
$\int_a^bf(t)dt$. BUT when we are given a generalized function $D(t)$
we {\it only} have at our disposal the latter quantities.  In fact, a
generalized function of a real variable $D(t)$ is (formally) nothing
more than a {\it rule} that assigns to any finite interval $[a,b]$ ($a
\le b$) a quantity that we might denote $\int_a^bD(t)dt$ and that {\it
  behaves as if it were the integral of a function} and in
particular---for three real numbers $a\le b\le c$ we have the
additivity relation
 $$  \int_a^cD(t)dt \ = \ \int_a^bD(t)dt \ + \ \int_b^cD(t)dt.$$
 
 SO, any honest function integrable over finite intervals clearly {\it
   is} a distribution (forget about its values!) but $\dots$ there are
 many more generalized functions, and including them in our sights
 gives us a very important tool.
 
 
 It is natural to talk, as well, of Cauchy sequences, and limits, of
 distributions. We'll say that such a sequence $D_1(t), D_2(t),
 D_3(t),\dots$ is a {\bf Cauchy sequence} if for every interval
 $[a,b]$ the quantities $$\int_a^bD_1(t)dt,\quad
 \int_a^bD_2(t)dt,\quad\int_a^bD_3(t)dt,\dots$$ form a Cauchy sequence
 of real numbers. Now, any Cauchy sequence of distributions {\it
   converges to a limiting distribution} $D(t)$ which is defined by
 the rule that for every interval $[a,b]$,
  
  $$ \int_a^bD(t)dt\ =\ \lim_{i \to \infty} \int_a^bD_i(t)dt.$$
    
  
  If, by the way, you have an infinite sequence---say---of honest,
  continuous, functions that converges uniformly to a limit (which
  will again be a continuous function) then that sequence certainly
  converges---in the above sense---to the same limit when these
  functions are viewed as generalized functions. BUT, there are many
  important occasions where your sequence of honest continuous
  functions doesn't have that convergence property and {\it yet} when
  they are viewed as generalized functions they do converge to some
  generalized function as a limit. We will see this soon when we get
  back to the ``sequence of the red graphs." This sequence {\bf does}
  converge (in the above sense) to the delta-function when these red
  graphs are thought of as a sequence of generalized functions.




 The integral notation for distribution is very useful, and allows us the flexibility to define, for nice enough---and honest---functions $c(t)$ useful expressions such as $$\int_a^bc(t)D(t).$$ 

 \ill{dirac_delta}{0.5}{The Dirac $\delta$-``function'' (actually
   distribution), where we draw a vertical arrow to illustrate the
   delta function with support at a given point.}

 For example, for the Dirac $\delta$-function we have been discussing
 (i.e., the limit of the red graphs of the previous section) {\it is}
 an honest function away from $t=0$ and ---in fact---is the ``trivial
 function" zero away from the origin.  And at $0$, we may {\it say}
 that it has the ``value" infinity, in honor of it being the limit of
 blip functions getting taller and taller at $0$ but the feature that
 pins it down as a distribution is given by its behavior relative to
 the second feature above, the area of its graph over the open
 interval between $a$ and $b$:

\begin{itemize}
\item If both $a$ and $b$ have the same sign (i.e., if the origin is
  not in the open interval spanned by $a$ and $b$) then the ``area
  under the graph of our Dirac $\delta$-function" is $0$.
\item If $a$ is negative and $b$ is positive then the ``area under the
  graph of our Dirac $\delta$-function" is $1$---in
  notation $$\int_a^b\delta = 1.$$
\end{itemize}




We sometimes summarize the fact that these areas vanish so long as the
origin is not included in the interval we are considering by saying
that the {\bf support} of this $\delta$-function is ``at the origin.''
 
 
Once you're happy with {\it this} Dirac $\delta$-function, you'll also
be happy with a Dirac $\delta$-function---call it $\delta_x$---with
support concentrated at any specific real number $x$ gotten by
``translating" the one we've been talking about appropriately;
$\delta_x$ vanishes for $t \ne x$ and intuitively speaking, has an
{\it infinite blip} at $t=x$.

So, the original delta-function we were discussing, i.e., $\delta(t)$
would be denoted $\delta_0(t)$.
  
 \noindent {\bf A question:} If you've never seen distributions
 before, but know the Riemann integral, can you guess at what the
 definition of $\int_a^bc(t)D(t)$ is, and can you formulate hypotheses
 on $c(t)$ that would allow you to endow this expression with a
 definite meaning?


\noindent {\bf A second question:} if you have not seen distributions
before, and have answered the first question above, let
$c(t)$ be an honest function for which your definition
of $$\int_a^bc(t)D(t)$$ applies. Now let $x$ be a real number.
 Can you use your definition to compute
 
 $$\int_{-{\infty}}^{+{\infty}}c(t)\delta_x(t)?$$
 
 The answer, by the way, is: 
 $\int_{-{\infty}}^{+{\infty}}c(t)\delta_x(t)=c(x).$ This will be useful in the later sections!     
     

The theory of distributions gives a partial answer to the following funny question:
     

\begin{quote}  How  in the world can you ``take the derivative'' of a function $F(t)$ that doesn't have a derivative?
\end{quote}

The short answer to this question is that {\it this derivative $F'(t)$
  which doesn't exist as a function may exist as a distribution.}
What then is the integral of that distribution? Well, it is given by
the original function!
     
$$\int_a^bF'(t)dt \ = \ F(b) -F(a).$$
     
Let us practice this with simple staircase functions. For example,
what is the {\it derivative}---in the sense of the theory of
distributions---of the function in Figure~\ref{fig:simple_staircase}.
{\bf Answer} $\delta_0 + 2 \delta_1$.
     
 
\ill{simple_staircase}{.5}{The staircase function that is $0$ for $t
  \le 0$, $1$ for $0 <t \le 1$ and $3$ for $1< t \le 2$ has derivative
  $\delta_0 + 2\delta_1$.\label{fig:simple_staircase}}
     
      
We'll be dealing with much more complicated staircase functions in the
next section, but the general principles discussed here will nicely
apply there.\endnote{ From Wikipedia:
  \begin{quote} ``Generalized functions'' were introduced by Sergei
    Sobolev in 1935. They were independently introduced in the late
    1940s by Laurent Schwartz, who developed a comprehensive theory of
    distributions.
\end{quote}
}


\chapter{Fourier transforms: second visit}

In Part~\ref{part1} of this book  (in Chapter~\ref{sec:fourier1} above) we wrote:

\begin{quote} The operation that starts with a graph and goes to its
  spectral picture that records the frequencies, amplitudes, and
  phases of the pure sine waves that, together, compose the graph is
  called the {\bf Fourier transform}.
\end{quote}


  Now let's take a closer look at this operation {\it Fourier transform}.
  
  We will focus our discussion on an {\bf even} function $f(t)$ of a
  real variable $t$.  ``{\bf Even}" means that its graph is symmetric
  about the $y$-axis; that is, $f(-t)= f(t)$.  See
  Figure~\ref{fig:even_function}.
  
  \ill{even_function}{.8}{The graph of an even function is symmetrical
    about the $y$-axis.\label{fig:even_function}}
  
  When we get to apply this discussion to the {\it staircase of
    primes} $\pi(t)$ or the {\it tinkered staircase of primes}
  $\psi(t)$ both of which being defined only for positive values of
  $t$, then we would ``lose little information" in our quest to
  understand them if we simply ``symmetrized their graphs'' by
  defining their values on negative numbers $-t$ via the formulas
  $\pi(-t)=\pi(t)$ and $\psi(-t)=\psi(t)$ thereby turning each of them
  into {\it even functions}.

\ill{even_pi}{.8}{Even extension of the staircase of primes.}

The idea behind the Fourier transform is to express $f(t)$ as {\it
  made up out of sine and cosine wave functions}.  Since we have
agreed to consider only even functions, we can dispense with the sine
waves---they won't appear in our Fourier analysis---and ask how to
reconstruct $f(t)$ as a {\it sum} (with coefficients) of cosine
functions (if only finitely many frequencies occur in the spectrum of
our function) or more generally, as an {\it integral} if the spectrum
is more elaborate.  For this work, we need a little machine that tells
us, for each real number $\theta$ whether or not $\theta$ is in the
spectrum of $f(t)$, and if so, what the amplitude is of the cosine
function $\cos(\theta t)$ that occurs in the Fourier expansion of
$f(t)$---this amplitude answers the awkwardly phrased question: {\it
  how much $\cos(\theta t)$ ``occurs in" $f(t)$?}  We will denote this
amplitude by ${\hat f}(\theta)$, and refer to it as {\bf the Fourier
  transform} of $f(t)$.  The {\bf Spectrum}, then, of $f(t)$ is the
set of all frequencies $\theta$ where the amplitude is nonzero.
  
  
\ill{fourier_machine}{.6}{The Fourier Transform Machine, which transforms $f(t)$ into ${\hat f}(\theta)$\label{fig:fourier_machine}}

    
Now in certain easy circumstances---specifically, if
$\int_{-{\infty}}^{+{\infty}}|f(t)|dt$ (exists, and) is finite---the
Integral Calculus provides us with an easy construction of that
machine (see Figure~\ref{fig:fourier_machine}); namely:
      
    $${\hat f}(\theta) = \int_{-{\infty}}^{+{\infty}}f(t)\cos(-\theta t)dt.$$
     
 
    This concise machine manages to ``pick out'' just the part of
    $f(t)$ that has frequency $\theta$!!  It provides for us the {\it
      analysis} part of the Fourier analysis of our function
    $f(t)$. But there is a {\it synthesis} part to our work as well,
    for we can reconstruct $f(t)$ from its Fourier transform, by a
    process intriguingly similar to the analysis part; namely:
       
      $$f(t)  = {\frac{1}{2\pi}}\int_{-{\infty}}^{+{\infty}}{\hat f}(\theta)\cos(\theta t)d\theta.$$
       
      We are not so lucky to have
      $\int_{-{\infty}}^{+{\infty}}|f(t)|dt$ finite when we try our
      hand at a Fourier analysis of the staircase of primes, but we'll
      work around this!
  
  

\chapter[Fourier transform of delta]{What is the Fourier transform of a delta function?\label{sec:ftdelta}}

Consider the $\delta$-function that we denoted $\delta(t)$ (or
$\delta_0(t)$). This is also the ``generalized function" that we
thought of as the ``limit of the red graphs" in Chapter~\ref{sec:dist}
above. Even though $\delta(t)$ is a distribution and {\it not} a bona
fide function, it is symmetric about the origin, and
also $$\int_{-{\infty}}^{+{\infty}}|\delta(t)|dt$$ exists, and is
finite (its value is, in fact, $1$). All this means that,
appropriately understood, the discussion of the previous section
applies, and we can {\it feed} this delta-function into our Fourier
Transform Machine (Figure~\ref{fig:fourier_machine}) to see what
frequencies and amplitudes arise in our attempt to express---whatever
this means!---the delta-function as a sum, or an integral, of cosine
functions.
   
   
     So what is the Fourier transform,  ${\hat \delta_0}(\theta)$, of the delta-function?
     
          
Well, the general formula would give us:
  $$ {\hat \delta_0}(\theta) = \int_{-\infty}^{+\infty}\cos(-\theta t)\delta_0(t)dt$$
  and as we mentioned in section 18, for any nice function $c(t)$ we
  have that the integral of the product of $c(t)$ by the distribution
  $\delta_x(t)$ is given by the {\it value} of the function $c(t)$ at
  $t=x$.  SO:
  
$$ {\hat \delta_0}(\theta) = \int_{-\infty}^{+\infty}\cos(-\theta t)\delta_0(t)dt = \cos(0) = 1.$$
    
    
In other words, the Fourier transform of $\delta_0(t)$ is the constant
function $$ {\hat \delta_0}(\theta)=1.$$ One can think of this
colloquially as saying that the delta-function is a perfect example of
{\it white noise} in that {\it every} frequency occurs in its Fourier
analysis and they all occur in equal amounts.
        
To generalize this computation let us consider for any real number $x$
the symmetrized delta-function with support at $x$ and $-x$, given
by $$d_x(t) \ = \ (\delta_x(t) + \delta_{-x}(t))/2$$
    
    
    \ill{two_delta}{0.4}{The sum $(\delta_x(t) + \delta_{-x}(t))/2$, where we draw vertical arrows to illustrate the Dirac delta functions.}
    
    What is the Fourier transform of this $d_x(t)$?  The answer is
    given by making the same computation as we've just made:
    
\begin{align*}
{\hat d_x}(\theta)  &=  {\frac{1}{2}}\left(\int_{-\infty}^{+\infty}\cos(-\theta t)\delta_x(t)dt + \int_{-\infty}^{+\infty}\cos(-\theta t)\delta_{-x}(t)dt\right)\\
    &= {\frac{1}{2}}\big(\cos(-\theta x)+ \cos(+\theta x)\big)\\
    &= \cos(x\theta)
\end{align*}

    
To summarize this in ridiculous (!) colloquial terms: {\it for any
  frequency $\theta$ the amount of $\cos(\theta t)$ you need to build
  up the generalized function $(\delta_x(t) + \delta_{-x}(t))/2$ is
  $\cos(x\theta).$ }

    
So far, so good, but remember that the theory of the Fourier transform
has---like much of mathematics---two parts: an {\it analysis part} and
a {\it synthesis} part.  We've just perfomed the {\it analysis} part
of the theory for these symmetrized delta functions $(\delta_x(t) +
\delta_{-x}(t))/2$.

Can we synthesize them---i.e., build them up again---from their Fourier transforms?
  
  
  We'll leave this, at least for now, as a question for you. 

\part{The Spectrum of the Prime Numbers\label{part3}}


\chapter{On losing no information\label{sec:loseno}}

To manage to repackage the ``same'' data in various ways---where each
way brings out some features that would be kept in the shadows if the
data were packaged in some different way---is a high art, in
mathematics. In a sense {\it every} mathematical equation does this,
for the ``equal sign'' in the middle of the equation tells us that
even though the two sides of the equation may seem different, or have
different shapes, they are nonetheless ``the same data.''  For
example, the equation
  
  $$\log(XY) = \log(X) + \log(Y)$$

  which we encountered earlier in Chapter~\ref{sec:firstguess}, is
  just two ways of looking at the same thing, yet it was the basis for
  much manual calculation for several centuries.
  
  
  
  
  Now, the problem we have been concentrating on, in this book, has
  been---in effect---to understand the pattern, if we can call it
  that, given by the placement of prime numbers among the natural
  line-up of all whole numbers.
 
 \ill{primes_line}{1}{Prime Numbers up to $37$}
  
 There are, of course, many ways for us to present this basic
 pattern. Our initial strategy was to focus attention on the {\it
   staircase of primes} which gives us a vivid portrait, if you wish,
 of the order of appearance of primes among all numbers.
 
 \ill{PN_38}{.45}{Prime Numbers up to $37$}
 
 As we have already hinted in the previous sections, however, there
 are various ways open to us to tinker with---and significantly
 modify---our staircase {\it without losing the essential information
   it contains}. Of course, there is always the danger of modifying
 things in such a way that ``retrieval" of the original data becomes
 difficult.  Moreover, we had better remember every change we have
 made if we are to have any hope of retrieving the original data!
 
 With this in mind, let us go back to Chapter~\ref{sec:information}
 (discussing the staircase of primes) and Chapter~\ref{sec:tinkering},
 where we tinkered with the original staircase of primes---alias: the
 graph of $\pi(X)$---to get $\psi(X)$ whose risers look---from
 afar---as if they approximated the 45 degree staircase.
 
 
   At this point we'll do some further carpentry on $\psi(X)$:
   
  
   \begin{enumerate}
   
   \item Distort the $X$-axis of our staircase by replacing the
     variable $X$ by $e^t$ to get the function $$\Psi(t):=
     \psi(e^t).$$ No harm is done by this for we can retrieve our
     original $\psi(X)$ as $$\psi(X) = \Psi(\log(X)).$$ Our distorted
     staircase has risers at ($0$ and) all positive integral multiples
     of logs of prime numbers.
  

\illtwo{psi_38}{bigPsi_38}{.4}{The two staircases $\psi$ (left) and $\Psi$ (right).  Notice that the plot of $\Psi$ on the right is just the plot of $\psi$ but with the $X$-axis plotted on a logarithmic scale.}


\item Now we'll do something that might seem a bit more brutal: {\it
    take the derivative of this distorted staircase) $\Psi(t)$.}  This
  derivative $\Psi'(t)$ is a {\it generalized} function with support
  at all nonnegative integral multiples of logs of prime numbers.

  \ill{bigPsi_prime}{1}{$\Psi'(t)$ is a sum of Dirac delta functions
    at the logarithms of prime powers $p^n$ weighted by $\log(p)$ (and
    $\log(2\pi)$ at $0$).  The more red the arrow, the larger the
    weight.}

\item Now ---for normalization purposes---multiply $\Psi'(t)$ by the
  function $e^{-t/2}$ which has no effect whatsoever on the support.
  \end{enumerate}
    
Let us denote by $\Phi(t)$ the generalized function that resulted from the above carpentry:
  $$\Phi(t) \ = \ e^{-t/2}\Psi'(t)$$ 
  A distribution that has a discrete set of real numbers as its
  support---as $\Phi(t)$ does, we sometimes like to call {\bf spike
    distributions} since the pictures of functions approximating them
  tend to look like a series of spikes.
   
  We have then before us a spike distribution with support at integral
  multiples of logarithms of prime numbers, and this generalized
  function retains the essential information about the placement of
  prime numbers among all whole numbers, and will be playing a major
  role in our story.
   
 
   
%Here is a sequence of functions approximating it.

\ill{psi_200}{.6}{Illustration of the  staircase $\Psi(x)$  constructed in Chapter~\ref{sec:tinkering} that 
counts weighted prime powers.\label{fig:psi_200}}
  
So what happens when we take the derivative---in the sense of
distributions---of a complicated staircase?  For example, see
Figure~\ref{fig:psi_200}.  Well, we would have blip-functions (alias:
Dirac $\delta$-functions) at each point of discontinuity of $\Psi(x)$;
that is, at $x=$ any power of a prime.
  
     
   
%     \ill{psi_prime}{.7}{Continuous approximation to the staircase
%       $\Psi(x)$ (in red) along with a plot (in blue) of the
%       derivative of this approximation.}
     
%\ill{phi_50}{0.6}{An approximation to the distribution $\Phi(t)$}
     

The spike distribution $\Phi(t)$ is now a major player in our drama,
because all of the crucial valuable information of placement of primes
is contained in it: the ``blips'' constituting this distribution (its
support) would allow us to reconstruct the position of the prime
numbers among all numbers.

But there are many other ways to package this vital information, so we
must explain our motivation for subjecting our poor initial staircase
to the particular series of brutal acts of distortion that we
described, that end up with the distribution $\Phi(t)$.

We do this because we are fascinated by this list of very surprising
facts about this $\Phi(t)$, a distribution that contains full
information about the placement of prime numbers among all numbers.



\chapter{Going from the primes to the spectrum}

{\bf This chapter has not been fully written and there are some
mathematical issues that we don't yet understand, and we will  need to
work out in order to do a satisfactory job here.  We'll be  figuring
out how to deal with this.}
 
 
 
We've discussed the nature of the Fourier transform of (symmetrized)
$\delta$-functions in Chapter~\ref{sec:ftdelta}. In particular, recall
the ``spike function" $$d_x(t) \ = \ (\delta_x(t) +
\delta_{-x}(t))/2$$ that has support at the points $x$ and $-x$.  We
mentioned that its Fourier transform, ${\hat d}_x(\theta)$, is equal
to $\cos(x\theta)$ (and gave some hints about why this may be true).
  

Our next goal is to work with the much more interesting ``spike
function" $$\Phi(t) \ = \ e^{- t/2}\Psi'(t),$$ which was one of the
generalized function that we engineered in Chapter~\ref{sec:loseno},
and that has support at all nonnegative integral multiples of
logarithms of prime numbers.
   


   
As with any function---or generalized function---defined for
non-negative values of $t$, we can ``symmetrize it" (about the
$t$-axis) which means that we can define it on negative real numbers
by the equation
  $$\Phi(-t) = \Phi(t).$$  
  Let us make that convention, thereby turning $\Phi(t)$ into an {\it
    even} generalized function.

  \ill{bigPhi}{1}{$\Phi(t)$ is a sum of Dirac delta functions at the
    logarithms of prime powers $p^n$ weighted by $p^{-n/2}\log(p)$
    (and $\log(2\pi)$ at $0$).  The more blue the arrow, the smaller
    the weight.}

  
   We may want to think of $\Phi(t)$ as a limit of the following sequence of distributions,
 
 $$\Phi(t)\ = \ \lim_{C \to {\infty}}\Phi_{\le C}(t)$$ where $\Phi_{\le C}(t)$  is the finite linear combinations of (symmetrized) $\delta$-functions  $d_x(t)$:
 
 $$\Phi_{\le C}(t)\quad :=\quad  2\sum_{{\rm prime\ powers \ }p^n  \le C} p^{-n/2}\log(p)d_{n\log(p)}(t).$$
  
 
 Since the Fourier transform of each $d_{n\log(p)}(t)$ is
 $\cos(n\log(p)\theta)$ we compute the Fourier transform of $\Phi_{\le
   C}(t)$ to be:
  
  $${\hat \Phi}_{\le C}(\theta):= 2\sum_{{\rm prime\ powers\ }p^n <C} p^{-n/2} \log(p)\cos(n\log(p)\theta).$$
  
  
  We have come then---again---to the expressions graphed at the end of Part~\ref{part1}, in 
  Chapter~\ref{sec:fourier_staircase}.
  
  
  \ill{phihat_even_all}{1}{Plots of ${\hat \Phi}_{\le C}(\theta)$ for $C=5$ (top), 20, 50, and 500 (bottom).}

 \vskip5pt
  {\bf 
    \begin{itemize}
    \item The main mathematical task we have is to figure out whether---and in what sense---the Fourier transform of $${\Phi}_{\rm even}(t)$$ is
  \begin{itemize}
  \item discretely supported at $$\theta_1,\theta_2,\theta_3,\ldots$$
  \item a kind of limit of the ${\hat \Phi}_{\le C}(\theta)$ as $ C$ goes to $\infty$.
  \end{itemize}
    \vskip5pt
    \item The above discussion will formally {\it define} {\bf The Spectrum} so we can---at this point---put the following material into our chapter:
      \end{itemize}
}
    
 We can{ compute}---at least approximately---the $\theta_i$'s of this spectrum, the first few $\theta_i$'s being:
\begin{eqnarray*}
\theta_1 &=& 14.134725 \dots\\
\theta_2 &=& 21.022039 \dots\\
\theta_3 &=& 25.010857 \dots\\
\theta_4 &=& 30.424876 \dots\\
\theta_5 &=& 32.935061 \dots\\
\theta_6 &=& 37.586178 \dots
\end{eqnarray*}

Riemann defined this sequence of numbers in his 1859 article in a
manner somewhat different from the treatment we have given (in that
article these $\theta_i$ appear as the ``imaginary parts of the
nontrivial zeroes of his $\zeta$-function;" we will discuss this
briefly in Chapter 24 below). Riemann wrote:
 \begin{quote}
  ``One now finds indeed approximately this number of real roots
  within these limits, and it is very probable that all roots are
  real. Certainly one would wish for a stricter proof here; I have
  meanwhile temporarily put aside the search for this after some
  futile attempts, as it appears unnecessary for the next objective of
  my investigation.''
\end{quote}

Nowadays, these mysterious numbers, these spectral lines for the
staircase of primes are known to great accuracy.  Here is the smallest
one, $\theta_1$, given with over $1,\!000$ digits of its decimal
expansion:  
\vskip20pt
$14.134725141734693790457251983562470270784257115699243175685567460149\newline
9634298092567649490103931715610127792029715487974367661426914698822545\newline
8250536323944713778041338123720597054962195586586020055556672583601077\newline
3700205410982661507542780517442591306254481978651072304938725629738321\newline
5774203952157256748093321400349904680343462673144209203773854871413783\newline
1735639699536542811307968053149168852906782082298049264338666734623320\newline
0787587617920056048680543568014444246510655975686659032286865105448594\newline
4432062407272703209427452221304874872092412385141835146054279015244783\newline
3835425453344004487936806761697300819000731393854983736215013045167269\newline
6838920039176285123212854220523969133425832275335164060169763527563758\newline
9695376749203361272092599917304270756830879511844534891800863008264831\newline
2516911271068291052375961797743181517071354531677549515382893784903647\newline
4709727019948485532209253574357909226125247736595518016975233461213977\newline
3160053541259267474557258778014726098308089786007125320875093959979666\newline
60675378381214891908864977277554420656532052405$
\vskip20pt
\noindent{}and if, by any chance, you wish to peruse the first
$2,\!001,\!052$
 of these $\theta_i$'s lovingly calculated to an accuracy
within $3\cdot 10^{-9}$, consult Andrew Odlyzko's tables:
\url{http://www.dtc.umn.edu/~odlyzko/zeta_tables}

  
  

 
\chapter{How to build $\Phi(t)$ from the spectrum}
  \vskip5pt 
 {\bf The main mission of this chapter is to begin the discussion---in
the style of our book (!)---of Riemann's Explicit Formula,  and also
to explain and exhibit data for:}
   \vskip5pt
 \begin{center}
      \shadowbox{ \begin{minipage}{0.9\textwidth}
      \mbox{}       \vspace{0.2ex}
      \begin{center}{\bf\large The \RH{} is equivalent to the assertion that  $\Phi(t)$ is {\em somehow related to---} the distribution defined as the summation $1+\sum_{i=1}^{\infty}\cos(\theta_i t)$}  where the $\theta_i$'s run through the spectrum of the primes. \end{center}
...
    \vspace{1ex}
    \end{minipage}}
 \end{center}
  \vskip5pt
{\bf At the moment all we have done are these graphs:}
   \vskip5pt


\ill{phi_cos_sum_2_30_1000}{.8}{Illustration of $-\sum_{i=1}^{1000} \cos(\log(s)\theta_i)$, where
$\theta_1 \sim 14.13, \ldots$ are the first $1000$ frequencies.  The red
dots are at the prime powers $p^n$, whose size is proportional to $\log(p)$.}

\ill{phi_cos_sum_26_34_1000}{.8}{Illustration of $-\sum_{i=1}^{1000} \cos(\log(s)\theta_i)$ in the
neighborhood of a twin prime.  Notice how the two primes $29$ and $31$ are separated out
by the Fourier series, and how the prime powers $3^3$ and $2^5$ also appear.}

\ill{phi_cos_sum_1010_1026_15000}{.7}{Fourier series from $1,000$ to $1,030$ using 15,000 of the numbers $\theta_i$.  Note the twin primes $1019$ and $1021$ and that $1024=2^{10}$.}

 



\chapter[Building $\pi(X)$ knowing the Spectrum]{How to build $\pi(X)$ knowing the Spectrum (Riemann's way)}


  We have been dealing in this Part~\ref{part2} of our book with $\Phi(t)$ a
  distribution that---we said---contains all the essential information
  about the placement of primes among numbers. We have given a clean
  restatement of Riemann's hypothesis, the third restatement so far,
  in term of this $\Phi(t)$.  But $\Phi(t)$ was the effect of a series
  of recalibrations and reconfigurings of the original untampered-with
  staircase of primes.  A test of whether we have strayed from our
  original problem---to understand this staircase---would be whether
  we can return to the original staircase, and ``reconstruct it'' so to
  speak, solely from the information of $\Phi(t)$---or equivalently,
  assuming the \RH{} as formulated in the previous
  section---can we construct the staircase of primes $\pi(X)$ solely
  from knowledge of the sequence of real numbers $\theta_1,
  \theta_2,\theta_3,\dots$
  
  


The answer to this  is yes (given the \RH{}), and is discussed very beautifully  by Bernhard
Riemann himself in his famous 1859 article cited above.

Bernhard Riemann used the spectrum of the prime numbers to provide an 
exact analytic formula that analyzes and/or synthesizes the staircase of primes.  This formula is
motivated by Fourier's analysis of functions as constituted out of
sines.
Riemann started with a specific smooth function, which we will refer to
as $R(X)$, a function that Riemann offered, just as Gauss offered his
$\Li(X)$, as a candidate smooth function approximating the staircase of
primes.  
Recall from Chapter~\ref{sec:rh1} that Gauss's guess is $\Li(X)=
    \int_2^{X}dt/{\rm log}(t).$ 
Riemann's guess for a better approximation to $\pi(X)$ is obtained
from Gauss's using the Moebius function $\mu(n)$, which is defined by
$$
 \mu(n) = \begin{cases} 
    1 & 
       \mbox{if $n$ is a square-free positive integer with an even number of distinct prime factors}\\
    -1& \mbox{if $n$ is a square-free positive integer with an odd number of distinct prime factors}\\ 
    0 & \mbox{if $n$ is not square-free.} 
 \end{cases} 
 $$
 See Figure~\ref{fig:moebius} for a plot of the Moebius function. 
 
\ill{moebius}{1}{The blue dots plot the values of the Moebius function $\mu(n)$\label{fig:moebius}}


Riemann's guess is
$$
R(X) = \sum_{n=1}^{\infty}{\mu(n)\over n} \Li(X^{1\over n}),
$$  
where $\mu(n)$ is the Moebius function introduced above. 

\ill{riemann_RX}{0.8}{Riemann defining $R(X)$ in his manuscript}

In Chapter~\ref{sec:pnt} we encountered the Prime Number Theorem which
asserts that $X/\log(X)$ and $\Li(X)$ are both approximation for
$\pi(X)$, in the sense that both go to infinity at the same rate.
8Our first formulation of the \RH{} (see page~\pageref{rh:first}) was
that $\Li(X)$ is an essentially square root accurate approximation of
$\pi(X)$.  Figures~\ref{fig:guess100}--\ref{fig:guess10000} illustrate
that Riemann's function $R(X)$ appears to be an even better
approximation to $\pi(X)$ than anything we have seen before.

\illtwo{pi_riemann_gauss_100}{pi_riemann_gauss_1000}{0.5}{Comparisons of $\Li(X)$ (top), $\pi(X)$ (middle), and $R(X)$ (bottom)\label{fig:guess100}}

\ill{pi_riemann_gauss_10000-11000}{0.5}{Closeup comparison of  $\Li(X)$ (top), $\pi(X)$ (middle), and $R(X)$ (bottom)\label{fig:guess10000}}


Think of Riemann's smooth curve $R(X)$ as the {\em fundamental}
approximation to $\pi(X)$.
Riemann offered much more than just a (conjecturally) better 
approximation to $\pi(X)$ in his wonderful 1859 article.  
He found a way to construct what looks like a Fourier series,
but with $\sin(X)$ replaced by $R(X)$ and spectrum the $\theta_i$, which
conjecturally exactly equals $\pi(X)$.
He gave an infinite sequence of improved guesses,
$$
R(X) = R_0(X),\quad R_1(X), \quad R_2(X), \quad R_3(X), \quad \ldots
$$
and he hypothesized that one and all of them were all
essentially square root approximations to $\pi(X)$,
and that the sequence of these better and better approximations converge to give an exact formula
for $\pi(X)$.

Thus not only did Riemann provide a ``fundamental'' (that is, a smooth curve
that is an astoundingly close to $\pi(X)$) but he viewed this as just a
starting point, for he gave the recipe for providing an infinite
sequence of corrective terms---call them Riemann's {\em harmonics}; we
will denote the first of these ``harmonics'' $C_1(X)$, the second
$C_2(X)$, etc.  Riemann gets his first corrected curve, $R_1(X)$, from
$R(X)$ by adding this first harmonic to the fundamental, $$R_1(X) =
R(X) + C_1(X),$$ he gets the second by correcting $R_1(X)$ by adding
the second harmonic $$R_2(X) = R_1 (X) + C_2(X),$$ and so on $$R_3(X)
= R_2 (X) + C_3(X),$$ and in the limit provides us with an exact fit.

\ill{riemann_Rk}{0.8}{Riemann analytic formula for $\pi(X)$.}

The \RH{}, if true, would tell us that these correction
terms $C_1(X), C_2(X), C_3(X),\dots$ are all {\em square-root small},
and all the successively corrected smooth curves $$R(X), R_1(X),
R_2(X),R_3(X),\dots$$ are good approximations to $\pi(X)$.
Moreover, 
$$
 \pi(X) = R(X) + \sum_{k=1}^{\infty} C_k(X).
$$

The elegance of Riemann's treatment of this problem is that the
corrective terms $C_k(X)$ are all {\em modelled on} the fundamental
$R(X)$ and are completely described if you know the sequence of real
numbers $\theta_1, \theta_2, \theta_3,\dots$ of the last section.


Here in the book will be where we refer explicitly to complex numbers
for the first time!!  The definition of Riemann's $C_k(X)$ requires
complex numbers $a+bi$, where $i=\sqrt{-1}$, and requires extending
the definition of the function $\Li(X)$ to make sense when given
complex numbers as input.  Assuming the \RH{}, the Riemann correction
terms $C_k(X)$ are then
$$
   C_k(X)= -R(X^{\frac{1}{2} + i\theta_k}), 
$$
where $\theta_1 = 14.134725\dots, \theta_2 = 21.022039\dots$, etc.,
are the spectrum of the prime numbers.\endnote{ People who know that
  these correction terms are index by the nontrivial zeroes of the
  Riemann zeta-function may well ask how I propose to order them if RH
  is false; the following prescription will do: order them in terms of
  (the absolute value of) their imaginary part, and in the unlikely
  situation that there is more than one zero with the same imaginary
  part, order zeroes of the same imaginary part by their real parts,
  going from right to left.}

Riemann provided an extraordinary recipe that allows us to work
out the harmonics, $$C_1(X),\quad C_2(X),\quad C_3(X),\quad \dots$$ without our having
to consult, or compute with, the actual staircase of primes. As with
Fourier's modus operandi where both {\em fundamental} and all {\em
  harmonics} are modeled on the sine wave, but appropriately
calibrated, Riemann fashioned his higher harmonics, modeling them all
on a single function, namely his initial guess $R(X)$.

The convergence of $R_k(X)$ to $\pi(X)$ is strikingly illustrated
in the plots in Figures~\ref{fig:rkfirst}--\ref{fig:rklast} of $R_k$ for various values of $k$.
 
  
\ill{Rk_1_2_100}{.9}{The function $R_{1}$ approximating the staircase of primes up to $100$\label{fig:rkfirst}}

\ill{Rk_10_2_100}{.9}{The function $R_{10}$ approximating the staircase of primes up to $100$}

\ill{Rk_25_2_100}{.9}{The function $R_{25}$ approximating the staircase of primes up to $100$}

\ill{Rk_50_2_100}{.9}{The function $R_{50}$ approximating the staircase of primes up to $100$}


\ill{Rk_50_2_500}{.9}{The function $R_{50}$ approximating the staircase of primes up to $500$}

\ill{Rk_50_350_400}{.9}{The function $\Li(X)$ (top, green), the function $R_{50}(X)$ (in blue), and the staircase of primes on the interval from 350 to 400.\label{fig:rklast}}

\part{Back to Riemann\label{part4}}

\chapter[The Riemann Zeta-Function]{The Riemann Zeta-Function; and Riemann's Hypothesis (fourth version)}

Here we develop a tiny bit of the traditional route of exposition
  to the Riemann Hypothesis. Namely, in terms of the successive nontrivial {\em zeroes of
    Riemann's zeta-function}.
    
There are infinitely many of these
nontrivial$^4$ zeroes of the $ \zeta$-function. If you want to 
peruse the first
$2,\!001,\!052$
 of these $\theta_i$'s lovingly calculated to an accuracy
within $3\cdot 10^{-9}$,
consult Andrew Odlyzko's tables:
\url{http://www.dtc.umn.edu/~odlyzko/zeta\_tables/}. 

The first three of these zeroes are:
  $$\rho_1={1\over 2} + 14.134725\dots i\ \ \ \ \  \rho_2={1\over 2} + 21.022040\dots i\ \ \ \rho_3={1\over 2} + 25.01\dots i\ \ 
\ {\rm etc.}.]$$ 



We hope that it is OK if we don't explain this at all, for it does
require calculus but we can think of these ``zeroes'' as being a
certain precious infinite sequence of quantities, that contain within
them the secret of the exact determination of the pattern of prime
numbers. (You can squint at the way Riemann wrote out the
zeta-function by trying to make out the formula on the first page of
his manuscript, in Figure~\ref{fig:riemamn}.)
    



You will notice the curious persistence of the ${1\over 2}$'s in the
exponents of the three zeroes listed above.  Riemann called attention
to this phenomenon, which we now know holds for all 1029.9 billion
zeroes that have been tabulated as of Feb.  18, 2005!  The ${1\over
  2}$ in the exponent of a correction term is what guarantees that the
correction term is indeed ``square- root small.'' We are ready, then,
for another equivalent formulation of the \RH{}, this
being--in fact--the more traditional way of expressing it:
 


  \begin{center}
       \shadowbox{ \begin{minipage}{0.9\textwidth}
\mbox{}       \vspace{0.2ex}
       \begin{center}{\bf\large The {\bf \RH{}} (fourth formulation)}\end{center}
       \medskip
       The \RH{} is equivalent to the statement that all
       the nontrivial zeroes of the Riemann zeta-function lie on the
       line ${1\over 2}+iy $ in the complex plane
\vspace{1ex}
\end{minipage}}
      \end{center}





That a simple geometric property of these zeroes (lying on a line!) is
directly equivalent to such profound (and more difficult to express)
regularities among prime numbers suggests that these zeroes and the
parade of Riemann's corrections governed by them--when we truly
comprehend their message--may have lots more to teach us, may
eventually allow us a more powerful understanding of arithmetic.  This
infinite collection of complex numbers, i.e., the nontrivial zeroes of
the Riemann zeta function, plays a role with respect to $\pi(X)$ rather
like the role the {\em spectrum} of the Hydrogen atom, plays in
Fourier's theory.  Are the primes themselves no more than an
epiphenomenon, behind which there lies, still veiled from us--a
yet-to-be-discovered, yet-to-be-hypothesized, profound conceptual key
to their perplexing orneriness.  Are the many innocently posed, yet
unanswered, phenomenological questions about numbers--such as in the
ones listed earlier-- waiting for our discovery of this deeper level
of arithmetic?  Or for layers deeper still?  Are we, in fact, just at
the beginning?




These are not completely idle thoughts, for a tantalizing analogy
relates the number theory we have been discussing to an already
established branch of mathematics--due, largely, to the work of
Alexander Grothendieck, and Pierre Deligne--where the corresponding
analogue of Riemann's hypothesis has indeed been proved$\dots$


\chapter{Glossary}

Here we give all the connections with the standard literature and 
coventional terminology that we restrained
  ourselves from giving in the text itself.

  For the moment the list of entries is the following but it will
  expand.

  $\pi(X)= \pi_0(X)$, $Q(X)= \psi_0(X)$, $\log, \exp$, $\delta$,
  distributions, RSA cryptography, Mersenne prime, $\Li(x)$, random
  walk, spectrum, harmonic, fundamental, frequency, phase, amplitude,
  band-pass, complex numbers, complex plane, Riemann Zeta function,
  zeroes of zeta.


  Also mention Brian Conrey's Notices article on RH as ``among the
  best 12 pages of RH survey material that there is---at least for an
  audience of general mathematicians.''
  

\part{Epilogue\label{part5}}

\chapter{Theoretical Chapter proving that RH implies our formula,
  etc.   No holds bared}

\chapter{Computing}

All of the computations for this book were done using Sage, which is a
free open source matheamtical software package
(\url{http://www.sagemath.org}).  If you would like to replicate any
of the computations, install Sage, and the purplesage Python package,
which is available here \url{http://code.google.com/p/purplesage/}.
The code for the book is all in \verb|psage/rh/mazur_stein|.

In Chapter~\ref{ch:what_are_primes} we discussed primes first in the
context of integer factorization.  Given an integer $n$, there is an
array of algorithms available for trying to write $n$ as a product of
prime numbers.  First we can apply {\em trial division}, where we
simply divide $n$ by each prime $2, 3, 5, 7, 11, 13, \ldots$ in turn,
and see what small prime factors we find (up to a few digits).  After
using this method to eliminate as many primes as we have patience to
eliminate, we typically next turn to a technique called {\em Lenstra's
  elliptic curve method}, which allows us to check $n$ for
divisibility by bigger primes (e.g., around 10--15 digits).  Once
we've exhausted our patience using the elliptic curve method, we would
next hit our number with something called the {\em quadratic sieve},
which works well for factoring numbers of the form $n=pq$, with $p$
and $q$ primes of roughly equal size, and $n$ having less than 100
digits (say, though the 100 depends greatly on the implementation).
All of the above algorithms---and then some---are implemented in Sage,
and used by default when you type {\tt factor(n)} into Sage. Try
typing {\tt factor(some number, verbose=8)} to see for yourself.  

If the quadratic sieve fails, a final recourse is to run the {\em
  number field sieve} algorithm, possibly on a supercomputer.  To give
a sense of how powerful (or powerless, depending on perspective!)  the
number field sieve is, a record-setting factorization of a general
number using this algorithm is the factorization of a 232 digit number
called RSA-768:

$
n = 12301866845301177551304949583849627207728535695953347921973224521\\
517264005072636575187452021997864693899564749427740638459251925573263\\
034537315482685079170261221429134616704292143116022212404792747377940\\
80665351419597459856902143413
$
\par\noindent{}which factors as $pq$, where\par\noindent{}
$
 p = 334780716989568987860441698482126908177047949837137685689124313889\\
82883793878002287614711652531743087737814467999489
$
\par\noindent{}and\par\noindent{}
$
q = 367460436667995904282446337996279526322791581643430876426760322838\\
15739666511279233373417143396810270092798736308917.
$
\par\noindent{}I encourage you to try to 
factor $n$ in Sage, and see that it fails. 
Sage does not currently (as of 2011) include an implementation of the
number field sieve algorithm, though there are some free implementations
currently available (see \url{http://www.boo.net/~jasonp/qs.html}).

In contrast to the situation with factorization, testing integers of
this size (e.g., the primes $p$ and $q$) for primality is relatively
easy.  There are fast algorithms that can tell whether or not any
random thousand digit number is prime in a fraction of
second. Try for yourself using the Sage command \verb|is_prime|.
For example, if $p$ and $q$ are as above:
\begin{verbatim}
sage: is_prime(p)
True
sage: is_prime(q)
True
sage: is_prime(p*q)
False
sage: factor(p*q, verbose=8)
... watch Sage try and fail to factor n...
\end{verbatim}

Incidentally, we can use Sage to instantly compute the digits of the ``hefty
number'' $p = 2^{43,112,609}-1$ that arises in
Chapter~\ref{ch:what_are_primes}.
\begin{verbatim}
sage: time p = 2^43112609 - 1
CPU times: user 0.00 s, sys: 0.01 s, total: 0.01 s
\end{verbatim}
In what sense have we {\em computed} $p$?  Internally, $p$ is now
stored in base $2$ in the computer's memory; given the special form of
$p$ it is not surprising that it took no time to compute.  Much more 
challenging is to compute all the base 10 digits of $p$, which takes
a few seconds:
\begin{verbatim}
sage: time d = str(p)
CPU times: user 17.20 s, sys: 0.15 s, total: 17.35 s
Wall time: 17.35 s
\end{verbatim}
Here are the last 50 digits of $p$:
\begin{verbatim}
sage: d[-50:]
'79670729447921616491887478265780022181166697152511'
\end{verbatim}


Chapter~\ref{ch:sieves} is about enumerating primes.  In Sage, the
function \verb|prime_range| does this, e.g., 
\begin{verbatim}
sage: prime_range(50)
[2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47]
sage: prime_range(50,100)
[53, 59, 61, 67, 71, 73, 79, 83, 89, 97]
\end{verbatim}
Sage has little trouble enumerating the primes up to a hundred million:
\begin{verbatim}
sage: time v = prime_range(10^8)
CPU times: user 0.77 s, sys: 0.99 s, total: 1.76 s
\end{verbatim}
You can also enumerate primes up to a billion, but this will use
several gigabytes of memory, so be careful not to crash your computer
if you try this:
\begin{verbatim}
sage: time v = prime_range(10^9)
CPU times: user 7.77 s, sys: 42.18 s, total: 49.95 s
sage: len(v)
50847534
\end{verbatim}

You can also compute $\pi(x)$ directly, without enumerating all primes:
\begin{verbatim}
sage: time prime_pi(10^9)
CPU times: user 0.02 s, sys: 0.00 s, total: 0.02 s
Wall time: 0.05 s
50847534
\end{verbatim}
This is much faster since it uses some clever counting tricks to find the
number of primes without actually listing them all. 



\backmatter

\chapter{Endnotes}
\theendnotes
\vfill
\mbox{}


%\eject
%\addcontentsline{toc}{chapter}{Index}
%\printindex



\label{lastpage}

\end{document}
   
